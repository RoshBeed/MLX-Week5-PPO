PPO Training Flow - Step by Step
=================================

┌─────────────────────────────────────────────────────────────────────────────┐
│                              PPO Training Loop                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  STEP 1: Generate Sequences                                                 │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Input: Prompts                                                      │   │
│  │ Process: Policy model generates token sequences                     │   │
│  │ Output: Generated texts + token-level logprobs                      │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    ↓                                       │
│  STEP 2: Extract (State, Action) Pairs                                   │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Input: Generated sequences                                           │   │
│  │ Process: For each token t:                                           │   │
│  │   • State = prompt + tokens[0:t-1]                                  │   │
│  │   • Action = token[t]                                               │   │
│  │   • Policy logprob = π_policy(action|state)                        │   │
│  │   • Reference logprob = π_sft(action|state)                        │   │
│  │ Output: List of (s, a, logprob_policy, logprob_ref) tuples          │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    ↓                                       │
│  STEP 3: Compute KL Divergence                                           │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Input: Policy and reference logprobs                                │   │
│  │ Process: KL_t = logprob_policy - logprob_ref                        │   │
│  │ Output: KL divergence for each token                                │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    ↓                                       │
│  STEP 4: Get Sequence Rewards                                            │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Input: Full sequences (prompt + generated)                          │   │
│  │ Process: Reward model scores each sequence                          │   │
│  │ Output: Scalar reward for each sequence                             │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    ↓                                       │
│  STEP 5: Adjust Rewards with KL Penalty                                 │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Input: Sequence rewards + KL divergences                            │   │
│  │ Process: adjusted_reward = reward - β × KL                          │   │
│  │ Output: KL-adjusted rewards for each token                          │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    ↓                                       │
│  STEP 6: Add Value Predictions                                          │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Input: States (prompt + partial sequences)                          │   │
│  │ Process: Value model predicts V(s) for each state                   │   │
│  │ Output: Value predictions for each state                            │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    ↓                                       │
│  STEP 7: Compute TD Errors                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Input: Rewards, values, next values                                 │   │
│  │ Process: δ_t = r_t + γV(s_{t+1}) - V(s_t)                          │   │
│  │ Output: TD errors for each timestep                                 │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    ↓                                       │
│  STEP 8: Compute GAE Advantages                                         │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Input: TD errors                                                    │   │
│  │ Process: A_t = Σ(γλ)^i δ_{t+i} (backwards from T to t)             │   │
│  │ Output: GAE advantages for each timestep                            │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    ↓                                       │
│  STEP 9: Calculate Returns (Reward-to-go)                               │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Input: Advantages + values                                          │   │
│  │ Process: G_t = A_t + V(s_t)                                         │   │
│  │ Output: Return targets for value function                           │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    ↓                                       │
│  STEP 10: Train Value Model                                            │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Input: States + return targets                                      │   │
│  │ Process: MSE loss between V(s) and G                                │   │
│  │ Output: Updated value function                                      │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    ↓                                       │
│  STEP 11: Train Policy Model (PPO-Clip)                                │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Input: States, actions, old logprobs, advantages                   │   │
│  │ Process:                                                           │   │
│  │   • Compute new logprobs π_new(action|state)                      │   │
│  │   • Calculate ratio = exp(logprob_new - logprob_old)               │   │
│  │   • PPO-clip loss = min(ratio×A, clip(ratio,1±ε)×A)               │   │
│  │ Output: Updated policy function                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

Key Mathematical Formulas:
==========================

1. KL Divergence: KL_t = log π_policy(a_t|s_t) - log π_sft(a_t|s_t)

2. Adjusted Reward: r'_t = R - β × KL_t

3. TD Error: δ_t = r'_t + γV(s_{t+1}) - V(s_t)

4. GAE Advantage: A_t = Σ_{i=0}^{T-t} (γλ)^i δ_{t+i}

5. Return: G_t = A_t + V(s_t)

6. Policy Ratio: ρ_t = π_new(a_t|s_t) / π_old(a_t|s_t)

7. PPO-Clip Loss: L = -E[min(ρ_t A_t, clip(ρ_t, 1-ε, 1+ε) A_t)]

8. Value Loss: L_v = MSE(V(s_t), G_t)

Hyperparameters:
================
• γ (gamma): Discount factor (default: 0.99)
• λ (lambda): GAE parameter (default: 0.95)
• β (beta): KL penalty coefficient (default: 0.1)
• ε (epsilon): PPO clip parameter (default: 0.2)
• Learning rates: Policy (1e-5), Value (1e-4) 