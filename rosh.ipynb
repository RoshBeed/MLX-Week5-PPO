{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2a74b6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,587,520 || all params: 600,637,440 || trainable%: 0.7638\n",
      "trainable params: 4,587,520 || all params: 600,637,440 || trainable%: 0.7638\n",
      "trainable params: 4,587,520 || all params: 600,637,440 || trainable%: 0.7638\n",
      "trainable params: 4,587,520 || all params: 600,637,440 || trainable%: 0.7638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rosh/MLX-Week5-PPO/.venv/lib/python3.13/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/Users/rosh/MLX-Week5-PPO/.venv/lib/python3.13/site-packages/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Qwen/Qwen3-0.6B-Base' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
      "  warnings.warn(\n",
      "/Users/rosh/MLX-Week5-PPO/.venv/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# setup base model\n",
    "model_name = \"Qwen/Qwen3-0.6B-Base\"  # Replace with Qwen3 if available\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "#setup lora config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # typical for attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "class SFTModel(nn.Module):\n",
    "    def __init__(self, base_model_name, lora_config):\n",
    "        super().__init__()\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "        self.model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, **kwargs)\n",
    "\n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.model.generate(*args, **kwargs)\n",
    "\n",
    "sft_model = SFTModel(model_name, lora_config)\n",
    "sft_model.model.print_trainable_parameters()\n",
    "sft_model.eval()\n",
    "\n",
    "#setup policy model\n",
    "class PolicyModel(nn.Module):\n",
    "    def __init__(self, lora_config=None):\n",
    "        super().__init__()\n",
    "        base_model = sft_model.model\n",
    "        self.model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, **kwargs)\n",
    "\n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.model.generate(*args, **kwargs)\n",
    "\n",
    "policy_model = PolicyModel(lora_config)\n",
    "policy_model.model.print_trainable_parameters()\n",
    "policy_model.eval()\n",
    "\n",
    "\n",
    "# setup reward model\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, lora_config):\n",
    "        super().__init__()\n",
    "        base_model = sft_model.model\n",
    "        self.model = get_peft_model(base_model, lora_config)\n",
    "        hidden_size = self.model.config.hidden_size\n",
    "        self.value_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        last_hidden = outputs.hidden_states[-1]\n",
    "        last_token_idx = attention_mask.sum(dim=1) - 1\n",
    "        last_token_idx = last_token_idx.unsqueeze(1).unsqueeze(2).expand(-1, 1, last_hidden.size(-1))\n",
    "        last_hidden_state = last_hidden.gather(1, last_token_idx).squeeze(1)\n",
    "        reward = self.value_head(last_hidden_state).squeeze(-1)\n",
    "        return reward\n",
    "    \n",
    "reward_model = RewardModel(lora_config)\n",
    "reward_model.model.print_trainable_parameters()\n",
    "reward_model.eval()\n",
    "\n",
    "#setup value model\n",
    "class ValueModel(nn.Module):\n",
    "    def __init__(self, lora_config):\n",
    "        super().__init__()\n",
    "        base_model = sft_model.model\n",
    "        self.model = get_peft_model(base_model, lora_config)\n",
    "        hidden_size = self.model.config.hidden_size\n",
    "        self.value_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        last_hidden = outputs.hidden_states[-1]\n",
    "        last_token_idx = attention_mask.sum(dim=1) - 1\n",
    "        last_token_idx = last_token_idx.unsqueeze(1).unsqueeze(2).expand(-1, 1, last_hidden.size(-1))\n",
    "        last_hidden_state = last_hidden.gather(1, last_token_idx).squeeze(1)\n",
    "        reward = self.value_head(last_hidden_state).squeeze(-1)\n",
    "        return reward\n",
    "    \n",
    "value_model = RewardModel(lora_config)\n",
    "value_model.model.print_trainable_parameters()\n",
    "value_model.eval()\n",
    "\n",
    "\n",
    "#util\n",
    "# Preprocessing function (outside the model)\n",
    "def preprocess_function(example, tokenizer, max_length=256):\n",
    "    return tokenizer(\n",
    "        example[\"prompt\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "# Collate function (outside the model)\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88f122b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated texts:\n",
      " A woman, whose \"other side\" boyfriend is\n",
      "sa pairs:\n",
      "Prompt 0 Step 0:\n",
      "State: 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:'\n",
      "Action token: ' A'\n",
      "Policy logprob: -4.208090782165527\n",
      "Reference logprob: -4.679380416870117\n",
      "KL (per token): 0.47128963470458984\n",
      "------------------------------\n",
      "Prompt 0 Step 1:\n",
      "State: 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A'\n",
      "Action token: ' woman'\n",
      "Policy logprob: -3.809088945388794\n",
      "Reference logprob: -4.732294082641602\n",
      "KL (per token): 0.9232051372528076\n",
      "------------------------------\n",
      "Prompt 0 Step 2:\n",
      "State: 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman'\n",
      "Action token: ','\n",
      "Policy logprob: -3.7089056968688965\n",
      "Reference logprob: -4.155608177185059\n",
      "KL (per token): 0.4467024803161621\n",
      "------------------------------\n",
      "Prompt 0 Step 3:\n",
      "State: 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman,'\n",
      "Action token: ' whose'\n",
      "Policy logprob: -3.7305712699890137\n",
      "Reference logprob: -4.212580680847168\n",
      "KL (per token): 0.4820094108581543\n",
      "------------------------------\n",
      "Prompt 0 Step 4:\n",
      "State: 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose'\n",
      "Action token: ' \"'\n",
      "Policy logprob: -5.3125319480896\n",
      "Reference logprob: -5.715484619140625\n",
      "KL (per token): 0.4029526710510254\n",
      "------------------------------\n",
      "Prompt 0 Step 5:\n",
      "State: 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"'\n",
      "Action token: 'other'\n",
      "Policy logprob: -4.6505045890808105\n",
      "Reference logprob: -5.410758972167969\n",
      "KL (per token): 0.7602543830871582\n",
      "------------------------------\n",
      "Prompt 0 Step 6:\n",
      "State: 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"other'\n",
      "Action token: ' side'\n",
      "Policy logprob: -1.672487735748291\n",
      "Reference logprob: -1.8130334615707397\n",
      "KL (per token): 0.14054572582244873\n",
      "------------------------------\n",
      "Prompt 0 Step 7:\n",
      "State: 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"other side'\n",
      "Action token: '\"'\n",
      "Policy logprob: -0.016949674114584923\n",
      "Reference logprob: -0.08494703471660614\n",
      "KL (per token): 0.06799736060202122\n",
      "------------------------------\n",
      "Prompt 0 Step 8:\n",
      "State: 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"other side\"'\n",
      "Action token: ' boyfriend'\n",
      "Policy logprob: -5.543594837188721\n",
      "Reference logprob: -5.786716461181641\n",
      "KL (per token): 0.24312162399291992\n",
      "------------------------------\n",
      "Prompt 0 Step 9:\n",
      "State: 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"other side\" boyfriend'\n",
      "Action token: ' is'\n",
      "Policy logprob: -0.7387890815734863\n",
      "Reference logprob: -1.0723142623901367\n",
      "KL (per token): 0.3335251808166504\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#PPO\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_token_level_sa_and_logprobs_with_ref(\n",
    "    policy_model, ref_model, tokenizer, prompts, device, max_new_tokens=10\n",
    "):\n",
    "    \"\"\"\n",
    "    For each prompt, generate a sequence and return:\n",
    "      - (state, action) pairs at each token step\n",
    "      - log-probability of each action under the policy model\n",
    "      - log-probability of each action under the reference (SFT) model\n",
    "    Returns:\n",
    "        all_sa_pairs: list of dicts, each with keys:\n",
    "            'state_text', 'action_token', 'action_token_id', 'logprob', 'ref_logprob', 'prompt_idx', 'step'\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = policy_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=1.0,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "    sequences = outputs.sequences  # (batch, prompt_len + max_new_tokens)\n",
    "    scores = outputs.scores        # list of length max_new_tokens, each (batch, vocab_size)\n",
    "    batch_size = sequences.shape[0]\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    generated_texts = []\n",
    "    all_sa_pairs = []\n",
    "    for i in range(batch_size):\n",
    "        gen_tokens = []\n",
    "        for t, score_t in enumerate(scores):\n",
    "            # State: prompt + previously generated tokens (up to t)\n",
    "            state_ids = sequences[i, :prompt_len + t]\n",
    "            state_text = tokenizer.decode(state_ids, skip_special_tokens=True)\n",
    "            # Action: next token\n",
    "            action_id = sequences[i, prompt_len + t].item()\n",
    "            action_token = tokenizer.decode([action_id])\n",
    "            gen_tokens.append(action_token)\n",
    "            # Logprob for this token (policy)\n",
    "            log_probs = F.log_softmax(score_t[i], dim=-1)\n",
    "            logprob = log_probs[action_id].item()\n",
    "            # Reference logprob\n",
    "            with torch.no_grad():\n",
    "                ref_inputs = state_ids.unsqueeze(0)\n",
    "                ref_outputs = ref_model(ref_inputs)\n",
    "                ref_logits = ref_outputs.logits  # (1, seq_len, vocab_size)\n",
    "            ref_next_token_logits = ref_logits[0, -1, :]\n",
    "            ref_log_probs = F.log_softmax(ref_next_token_logits, dim=-1)\n",
    "            ref_logprob = ref_log_probs[action_id].item()\n",
    "            all_sa_pairs.append({\n",
    "                \"state_text\": state_text,\n",
    "                \"action_token\": action_token,\n",
    "                \"action_token_id\": action_id,\n",
    "                \"logprob\": logprob,\n",
    "                \"ref_logprob\": ref_logprob,\n",
    "                \"prompt_idx\": i,\n",
    "                \"step\": t,\n",
    "            })\n",
    "        generated_texts.append(\"\".join(gen_tokens))\n",
    "    return all_sa_pairs, generated_texts\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy_model = policy_model.to(device)\n",
    "\n",
    "# Example batch of prompts\n",
    "prompts = [\n",
    "    \"SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:\",\n",
    "]\n",
    "sa_pairs, generated_texts = generate_token_level_sa_and_logprobs_with_ref(\n",
    "    policy_model, sft_model, tokenizer, prompts, device, max_new_tokens=10\n",
    ")\n",
    "\n",
    "print(\"generated texts:\")\n",
    "for text in generated_texts:\n",
    "    print(text)\n",
    "\n",
    "print(\"sa pairs:\")\n",
    "for pair in sa_pairs:\n",
    "    print(f\"Prompt {pair['prompt_idx']} Step {pair['step']}:\")\n",
    "    print(\"State:\", repr(pair['state_text']))\n",
    "    print(\"Action token:\", repr(pair['action_token']))\n",
    "    print(\"Policy logprob:\", pair['logprob'])\n",
    "    print(\"Reference logprob:\", pair['ref_logprob'])\n",
    "    print(\"KL (per token):\", pair['logprob'] - pair['ref_logprob'])\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "073a3e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 0 | Step 0\n",
      "  State:         'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:'\n",
      "  Action token:  ' A' (id: 362)\n",
      "  Policy logprob:    -4.2081\n",
      "  Reference logprob: -4.6794\n",
      "  KL divergence:     0.4713\n",
      "--------------------------------------------------\n",
      "Prompt 0 | Step 1\n",
      "  State:         'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A'\n",
      "  Action token:  ' woman' (id: 5220)\n",
      "  Policy logprob:    -3.8091\n",
      "  Reference logprob: -4.7323\n",
      "  KL divergence:     0.9232\n",
      "--------------------------------------------------\n",
      "Prompt 0 | Step 2\n",
      "  State:         'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman'\n",
      "  Action token:  ',' (id: 11)\n",
      "  Policy logprob:    -3.7089\n",
      "  Reference logprob: -4.1556\n",
      "  KL divergence:     0.4467\n",
      "--------------------------------------------------\n",
      "Prompt 0 | Step 3\n",
      "  State:         'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman,'\n",
      "  Action token:  ' whose' (id: 6693)\n",
      "  Policy logprob:    -3.7306\n",
      "  Reference logprob: -4.2126\n",
      "  KL divergence:     0.4820\n",
      "--------------------------------------------------\n",
      "Prompt 0 | Step 4\n",
      "  State:         'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose'\n",
      "  Action token:  ' \"' (id: 330)\n",
      "  Policy logprob:    -5.3125\n",
      "  Reference logprob: -5.7155\n",
      "  KL divergence:     0.4030\n",
      "--------------------------------------------------\n",
      "Prompt 0 | Step 5\n",
      "  State:         'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"'\n",
      "  Action token:  'other' (id: 1575)\n",
      "  Policy logprob:    -4.6505\n",
      "  Reference logprob: -5.4108\n",
      "  KL divergence:     0.7603\n",
      "--------------------------------------------------\n",
      "Prompt 0 | Step 6\n",
      "  State:         'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"other'\n",
      "  Action token:  ' side' (id: 3108)\n",
      "  Policy logprob:    -1.6725\n",
      "  Reference logprob: -1.8130\n",
      "  KL divergence:     0.1405\n",
      "--------------------------------------------------\n",
      "Prompt 0 | Step 7\n",
      "  State:         'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"other side'\n",
      "  Action token:  '\"' (id: 1)\n",
      "  Policy logprob:    -0.0169\n",
      "  Reference logprob: -0.0849\n",
      "  KL divergence:     0.0680\n",
      "--------------------------------------------------\n",
      "Prompt 0 | Step 8\n",
      "  State:         'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"other side\"'\n",
      "  Action token:  ' boyfriend' (id: 25838)\n",
      "  Policy logprob:    -5.5436\n",
      "  Reference logprob: -5.7867\n",
      "  KL divergence:     0.2431\n",
      "--------------------------------------------------\n",
      "Prompt 0 | Step 9\n",
      "  State:         'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"other side\" boyfriend'\n",
      "  Action token:  ' is' (id: 374)\n",
      "  Policy logprob:    -0.7388\n",
      "  Reference logprob: -1.0723\n",
      "  KL divergence:     0.3335\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def compute_kl_per_token(logprobs_policy, logprobs_ref):\n",
    "    \"\"\"\n",
    "    Compute per-token KL divergence for sampled actions.\n",
    "    Returns:\n",
    "        kl_per_token: torch.Tensor of shape (num_tokens,)\n",
    "    \"\"\"\n",
    "    logprobs_policy = torch.tensor(logprobs_policy)\n",
    "    logprobs_ref = torch.tensor(logprobs_ref)\n",
    "    kl_per_token = logprobs_policy - logprobs_ref\n",
    "    return kl_per_token\n",
    "\n",
    "# Suppose you have these from previous steps:\n",
    "# sa_pairs: list of dicts, each with 'logprob' (policy) and 'ref_logprob' (reference)\n",
    "logprobs_policy = [pair['logprob'] for pair in sa_pairs]\n",
    "logprobs_ref = [pair['ref_logprob'] for pair in sa_pairs]  # You'd add this key after SFT scoring\n",
    "\n",
    "kl_per_token = compute_kl_per_token(logprobs_policy, logprobs_ref)\n",
    "\n",
    "def add_kl_to_sa_pairs(sa_pairs):\n",
    "    for pair in sa_pairs:\n",
    "        logprob_policy = pair[\"logprob\"]\n",
    "        logprob_ref = pair[\"ref_logprob\"]\n",
    "        pair[\"kl_div\"] = logprob_policy - logprob_ref\n",
    "    return sa_pairs\n",
    "\n",
    "sa_pairs = add_kl_to_sa_pairs(sa_pairs)\n",
    "\n",
    "for pair in sa_pairs:\n",
    "    print(f\"Prompt {pair.get('prompt_idx', '?')} | Step {pair.get('step', '?')}\")\n",
    "    print(f\"  State:         {repr(pair['state_text'])}\")\n",
    "    print(f\"  Action token:  {repr(pair['action_token'])} (id: {pair['action_token_id']})\")\n",
    "    print(f\"  Policy logprob:    {pair['logprob']:.4f}\")\n",
    "    print(f\"  Reference logprob: {pair['ref_logprob']:.4f}\")\n",
    "    print(f\"  KL divergence:     {pair['kl_div']:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e609c8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: SUBREDDIT: r/relationships\n",
      "TITLE: Should I admit to snooping?\n",
      "POST: ...\n",
      "TL;DR: \n",
      "Generated text:  A woman, whose \"other side\" boyfriend is \n",
      "Reward = -0.6829\n"
     ]
    }
   ],
   "source": [
    "def compute_rewards_for_sequences(reward_model, tokenizer, prompts, generated_texts, device):\n",
    "    \"\"\"\n",
    "    For each prompt and generated text, compute the reward using the reward model.\n",
    "    Returns:\n",
    "        rewards: list of floats, one per sequence\n",
    "    \"\"\"\n",
    "    # Concatenate prompt and generated text for each example\n",
    "    full_texts = [p + gt for p, gt in zip(prompts, generated_texts)]\n",
    "    inputs = tokenizer(full_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        rewards = reward_model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "    # Handle both batch and single input cases\n",
    "    if isinstance(rewards, torch.Tensor):\n",
    "        rewards = rewards.squeeze()\n",
    "        if rewards.dim() == 0:\n",
    "            rewards = [rewards.item()]\n",
    "        else:\n",
    "            rewards = rewards.cpu().tolist()\n",
    "    elif isinstance(rewards, float):\n",
    "        rewards = [rewards]\n",
    "    return rewards\n",
    "\n",
    "rewards = compute_rewards_for_sequences(reward_model, tokenizer, prompts, generated_texts, device)\n",
    "for i, reward in enumerate(rewards):\n",
    "    print(f\"Prompt: {prompts[i]} \\nGenerated text: {generated_texts[i]} \\nReward = {reward:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b74cf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 0 | Step 0\n",
      "  Reward:           -0.6829\n",
      "  KL divergence:    0.4713\n",
      "  Adjusted reward:  -0.7301\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 1\n",
      "  Reward:           -0.6829\n",
      "  KL divergence:    0.9232\n",
      "  Adjusted reward:  -0.7753\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 2\n",
      "  Reward:           -0.6829\n",
      "  KL divergence:    0.4467\n",
      "  Adjusted reward:  -0.7276\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 3\n",
      "  Reward:           -0.6829\n",
      "  KL divergence:    0.4820\n",
      "  Adjusted reward:  -0.7311\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 4\n",
      "  Reward:           -0.6829\n",
      "  KL divergence:    0.4030\n",
      "  Adjusted reward:  -0.7232\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 5\n",
      "  Reward:           -0.6829\n",
      "  KL divergence:    0.7603\n",
      "  Adjusted reward:  -0.7590\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 6\n",
      "  Reward:           -0.6829\n",
      "  KL divergence:    0.1405\n",
      "  Adjusted reward:  -0.6970\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 7\n",
      "  Reward:           -0.6829\n",
      "  KL divergence:    0.0680\n",
      "  Adjusted reward:  -0.6897\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 8\n",
      "  Reward:           -0.6829\n",
      "  KL divergence:    0.2431\n",
      "  Adjusted reward:  -0.7073\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 9\n",
      "  Reward:           -0.6829\n",
      "  KL divergence:    0.3335\n",
      "  Adjusted reward:  -0.7163\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def adjust_rewards_with_kl(sa_pairs, rewards, kl_coef=0.1):\n",
    "    \"\"\"\n",
    "    For each (s, a) pair, adjust the reward by subtracting kl_coef * kl_div.\n",
    "    Broadcasts the sequence reward to each token in the sequence.\n",
    "    Modifies sa_pairs in-place and also returns it.\n",
    "    \"\"\"\n",
    "    for pair in sa_pairs:\n",
    "        reward = rewards[pair['prompt_idx']]\n",
    "        kl = pair['kl_div']\n",
    "        pair['adjusted_reward'] = reward - kl_coef * kl\n",
    "    return sa_pairs\n",
    "\n",
    "kl_coef = 0.1\n",
    "sa_pairs = adjust_rewards_with_kl(sa_pairs, rewards, kl_coef=kl_coef)\n",
    "\n",
    "# Print to verify\n",
    "for pair in sa_pairs:\n",
    "    print(f\"Prompt {pair['prompt_idx']} | Step {pair['step']}\")\n",
    "    print(f\"  Reward:           {rewards[pair['prompt_idx']]:.4f}\")\n",
    "    print(f\"  KL divergence:    {pair['kl_div']:.4f}\")\n",
    "    print(f\"  Adjusted reward:  {pair['adjusted_reward']:.4f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "96280e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 0 | Step 0\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:'\n",
      "  Action token:      ' A' (id: 362)\n",
      "  Policy logprob:    -4.2081\n",
      "  Reference logprob: -4.6794\n",
      "  KL divergence:     0.4713\n",
      "  Value prediction:  1.1618\n",
      "  KL-adjusted reward:-0.7301\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 1\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A'\n",
      "  Action token:      ' woman' (id: 5220)\n",
      "  Policy logprob:    -3.8091\n",
      "  Reference logprob: -4.7323\n",
      "  KL divergence:     0.9232\n",
      "  Value prediction:  1.0352\n",
      "  KL-adjusted reward:-0.7753\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 2\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman'\n",
      "  Action token:      ',' (id: 11)\n",
      "  Policy logprob:    -3.7089\n",
      "  Reference logprob: -4.1556\n",
      "  KL divergence:     0.4467\n",
      "  Value prediction:  1.2671\n",
      "  KL-adjusted reward:-0.7276\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 3\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman,'\n",
      "  Action token:      ' whose' (id: 6693)\n",
      "  Policy logprob:    -3.7306\n",
      "  Reference logprob: -4.2126\n",
      "  KL divergence:     0.4820\n",
      "  Value prediction:  1.6038\n",
      "  KL-adjusted reward:-0.7311\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 4\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose'\n",
      "  Action token:      ' \"' (id: 330)\n",
      "  Policy logprob:    -5.3125\n",
      "  Reference logprob: -5.7155\n",
      "  KL divergence:     0.4030\n",
      "  Value prediction:  1.7939\n",
      "  KL-adjusted reward:-0.7232\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 5\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"'\n",
      "  Action token:      'other' (id: 1575)\n",
      "  Policy logprob:    -4.6505\n",
      "  Reference logprob: -5.4108\n",
      "  KL divergence:     0.7603\n",
      "  Value prediction:  -0.0032\n",
      "  KL-adjusted reward:-0.7590\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 6\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"other'\n",
      "  Action token:      ' side' (id: 3108)\n",
      "  Policy logprob:    -1.6725\n",
      "  Reference logprob: -1.8130\n",
      "  KL divergence:     0.1405\n",
      "  Value prediction:  0.9319\n",
      "  KL-adjusted reward:-0.6970\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 7\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"other side'\n",
      "  Action token:      '\"' (id: 1)\n",
      "  Policy logprob:    -0.0169\n",
      "  Reference logprob: -0.0849\n",
      "  KL divergence:     0.0680\n",
      "  Value prediction:  0.4107\n",
      "  KL-adjusted reward:-0.6897\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 8\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"other side\"'\n",
      "  Action token:      ' boyfriend' (id: 25838)\n",
      "  Policy logprob:    -5.5436\n",
      "  Reference logprob: -5.7867\n",
      "  KL divergence:     0.2431\n",
      "  Value prediction:  1.6433\n",
      "  KL-adjusted reward:-0.7073\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 9\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"other side\" boyfriend'\n",
      "  Action token:      ' is' (id: 374)\n",
      "  Policy logprob:    -0.7388\n",
      "  Reference logprob: -1.0723\n",
      "  KL divergence:     0.3335\n",
      "  Value prediction:  1.1618\n",
      "  KL-adjusted reward:-0.7163\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def add_value_to_sa_pairs(sa_pairs, value_model, tokenizer, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    For each sa_pair, compute the value (predicted reward) using the value model,\n",
    "    and add it as 'value' to the sa_pair dict.\n",
    "    \"\"\"\n",
    "    # Collect all state_texts\n",
    "    state_texts = [pair['state_text'] for pair in sa_pairs]\n",
    "    values = []\n",
    "\n",
    "    # Process in batches for efficiency\n",
    "    for i in range(0, len(state_texts), batch_size):\n",
    "        batch_texts = state_texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_values = value_model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "        # batch_values: shape (batch,)\n",
    "        batch_values = batch_values.squeeze().cpu().tolist()\n",
    "        # Ensure batch_values is a list\n",
    "        if isinstance(batch_values, float):\n",
    "            batch_values = [batch_values]\n",
    "        values.extend(batch_values)\n",
    "\n",
    "    # Add to sa_pairs\n",
    "    for pair, value in zip(sa_pairs, values):\n",
    "        pair['value'] = value\n",
    "\n",
    "    return sa_pairs\n",
    "\n",
    "sa_pairs = add_value_to_sa_pairs(sa_pairs, value_model, tokenizer, device)\n",
    "\n",
    "# Print to verify\n",
    "for pair in sa_pairs:\n",
    "    print(f\"Prompt {pair['prompt_idx']} | Step {pair['step']}\")\n",
    "    print(f\"  State:             {repr(pair['state_text'])}\")\n",
    "    print(f\"  Action token:      {repr(pair['action_token'])} (id: {pair['action_token_id']})\")\n",
    "    print(f\"  Policy logprob:    {pair['logprob']:.4f}\")\n",
    "    print(f\"  Reference logprob: {pair['ref_logprob']:.4f}\")\n",
    "    print(f\"  KL divergence:     {pair['kl_div']:.4f}\")\n",
    "    print(f\"  Value prediction:  {pair['value']:.4f}\")\n",
    "    print(f\"  KL-adjusted reward:{pair['adjusted_reward']:.4f}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7876453d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 0 | Step 0\n",
      "  KL-adjusted reward: -0.7301\n",
      "  Value:              1.1618\n",
      "  TD error:           -0.8670\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 1\n",
      "  KL-adjusted reward: -0.7753\n",
      "  Value:              1.0352\n",
      "  TD error:           -0.5560\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 2\n",
      "  KL-adjusted reward: -0.7276\n",
      "  Value:              1.2671\n",
      "  TD error:           -0.4070\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 3\n",
      "  KL-adjusted reward: -0.7311\n",
      "  Value:              1.6038\n",
      "  TD error:           -0.5590\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 4\n",
      "  KL-adjusted reward: -0.7232\n",
      "  Value:              1.7939\n",
      "  TD error:           -2.5203\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 5\n",
      "  KL-adjusted reward: -0.7590\n",
      "  Value:              -0.0032\n",
      "  TD error:           0.1668\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 6\n",
      "  KL-adjusted reward: -0.6970\n",
      "  Value:              0.9319\n",
      "  TD error:           -1.2223\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 7\n",
      "  KL-adjusted reward: -0.6897\n",
      "  Value:              0.4107\n",
      "  TD error:           0.5263\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 8\n",
      "  KL-adjusted reward: -0.7073\n",
      "  Value:              1.6433\n",
      "  TD error:           -1.2004\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 9\n",
      "  KL-adjusted reward: -0.7163\n",
      "  Value:              1.1618\n",
      "  TD error:           -1.8781\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_td_errors(sa_pairs, gamma=0.99):\n",
    "    # 1. Group by prompt_idx\n",
    "    grouped = defaultdict(list)\n",
    "    for pair in sa_pairs:\n",
    "        grouped[pair['prompt_idx']].append(pair)\n",
    "\n",
    "    # 2. For each prompt, sort by step and compute TD error\n",
    "    for prompt_idx, pairs in grouped.items():\n",
    "        pairs = sorted(pairs, key=lambda x: x['step'])\n",
    "        for t, pair in enumerate(pairs):\n",
    "            reward = pair['adjusted_reward']\n",
    "            value = pair['value']\n",
    "            # Next value: value of next step, or 0 if last\n",
    "            if t < len(pairs) - 1:\n",
    "                next_value = pairs[t + 1]['value']\n",
    "            else:\n",
    "                next_value = 0.0\n",
    "            pair['td_error'] = reward + gamma * next_value - value\n",
    "    return sa_pairs\n",
    "\n",
    "sa_pairs = compute_td_errors(sa_pairs, gamma=0.99)\n",
    "\n",
    "# Print to verify\n",
    "for pair in sa_pairs:\n",
    "    print(f\"Prompt {pair['prompt_idx']} | Step {pair['step']}\")\n",
    "    print(f\"  KL-adjusted reward: {pair['adjusted_reward']:.4f}\")\n",
    "    print(f\"  Value:              {pair['value']:.4f}\")\n",
    "    print(f\"  TD error:           {pair['td_error']:.4f}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b27e402c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 0 | Step 0\n",
      "  TD error:   -0.8670\n",
      "  Advantage:  -6.3835\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 1\n",
      "  TD error:   -0.5560\n",
      "  Advantage:  -5.8656\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 2\n",
      "  TD error:   -0.4070\n",
      "  Advantage:  -5.6454\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 3\n",
      "  TD error:   -0.5590\n",
      "  Advantage:  -5.5699\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 4\n",
      "  TD error:   -2.5203\n",
      "  Advantage:  -5.3279\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 5\n",
      "  TD error:   0.1668\n",
      "  Advantage:  -2.9852\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 6\n",
      "  TD error:   -1.2223\n",
      "  Advantage:  -3.3514\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 7\n",
      "  TD error:   0.5263\n",
      "  Advantage:  -2.2638\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 8\n",
      "  TD error:   -1.2004\n",
      "  Advantage:  -2.9667\n",
      "------------------------------------------------------------\n",
      "Prompt 0 | Step 9\n",
      "  TD error:   -1.8781\n",
      "  Advantage:  -1.8781\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def compute_gae_advantages(sa_pairs, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    Compute GAE advantage for each (s, a) pair in sa_pairs.\n",
    "    Adds 'advantage' to each sa_pair.\n",
    "    Assumes sa_pairs are sorted by (prompt_idx, step).\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    grouped = defaultdict(list)\n",
    "    for pair in sa_pairs:\n",
    "        grouped[pair['prompt_idx']].append(pair)\n",
    "\n",
    "    for prompt_idx, pairs in grouped.items():\n",
    "        # Sort by step\n",
    "        pairs = sorted(pairs, key=lambda x: x['step'])\n",
    "        num_steps = len(pairs)\n",
    "        advantages = [0.0] * num_steps\n",
    "        gae = 0.0\n",
    "        # Go backwards through the sequence\n",
    "        for t in reversed(range(num_steps)):\n",
    "            td_error = pairs[t]['td_error']\n",
    "            gae = td_error + gamma * lam * gae\n",
    "            advantages[t] = gae\n",
    "        # Assign to sa_pairs\n",
    "        for t in range(num_steps):\n",
    "            pairs[t]['advantage'] = advantages[t]\n",
    "    return sa_pairs\n",
    "\n",
    "sa_pairs = compute_gae_advantages(sa_pairs, gamma=0.99, lam=0.95)\n",
    "\n",
    "for pair in sa_pairs:\n",
    "    print(f\"Prompt {pair['prompt_idx']} | Step {pair['step']}\")\n",
    "    print(f\"  TD error:   {pair['td_error']:.4f}\")\n",
    "    print(f\"  Advantage:  {pair['advantage']:.4f}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3479f550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 0 | Step 0\n",
      "  Value:         1.1618\n",
      "  Advantage:     -6.3835\n",
      "  Reward-to-go:  -5.2218\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 1\n",
      "  Value:         1.0352\n",
      "  Advantage:     -5.8656\n",
      "  Reward-to-go:  -4.8303\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 2\n",
      "  Value:         1.2671\n",
      "  Advantage:     -5.6454\n",
      "  Reward-to-go:  -4.3783\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 3\n",
      "  Value:         1.6038\n",
      "  Advantage:     -5.5699\n",
      "  Reward-to-go:  -3.9661\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 4\n",
      "  Value:         1.7939\n",
      "  Advantage:     -5.3279\n",
      "  Reward-to-go:  -3.5340\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 5\n",
      "  Value:         -0.0032\n",
      "  Advantage:     -2.9852\n",
      "  Reward-to-go:  -2.9884\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 6\n",
      "  Value:         0.9319\n",
      "  Advantage:     -3.3514\n",
      "  Reward-to-go:  -2.4195\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 7\n",
      "  Value:         0.4107\n",
      "  Advantage:     -2.2638\n",
      "  Reward-to-go:  -1.8531\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 8\n",
      "  Value:         1.6433\n",
      "  Advantage:     -2.9667\n",
      "  Reward-to-go:  -1.3234\n",
      "----------------------------------------\n",
      "Prompt 0 | Step 9\n",
      "  Value:         1.1618\n",
      "  Advantage:     -1.8781\n",
      "  Reward-to-go:  -0.7163\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def add_reward_to_go_to_sa_pairs(sa_pairs):\n",
    "    \"\"\"\n",
    "    For each sa_pair, add 'value_target' = advantage + value.\n",
    "    \"\"\"\n",
    "    for pair in sa_pairs:\n",
    "        pair['reward_to_go'] = pair['advantage'] + pair['value']\n",
    "    return sa_pairs\n",
    "\n",
    "sa_pairs = add_reward_to_go_to_sa_pairs(sa_pairs)\n",
    "\n",
    "for pair in sa_pairs:\n",
    "    print(f\"Prompt {pair['prompt_idx']} | Step {pair['step']}\")\n",
    "    print(f\"  Value:         {pair['value']:.4f}\")\n",
    "    print(f\"  Advantage:     {pair['advantage']:.4f}\")\n",
    "    print(f\"  Reward-to-go:  {pair['reward_to_go']:.4f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd4f34c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 0 | Step 0\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:'\n",
      "  Action token:      ' A' (id: 362)\n",
      "  Policy logprob:    -4.2081\n",
      "  Reference logprob: -4.6794\n",
      "  KL divergence:     0.4713\n",
      "  Value:             1.1618\n",
      "  KL-adjusted reward:-0.7301\n",
      "  TD error:          -0.8670\n",
      "  Advantage:         -6.3835\n",
      "  Reward-to-go:      -5.2218\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt 0 | Step 1\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A'\n",
      "  Action token:      ' woman' (id: 5220)\n",
      "  Policy logprob:    -3.8091\n",
      "  Reference logprob: -4.7323\n",
      "  KL divergence:     0.9232\n",
      "  Value:             1.0352\n",
      "  KL-adjusted reward:-0.7753\n",
      "  TD error:          -0.5560\n",
      "  Advantage:         -5.8656\n",
      "  Reward-to-go:      -4.8303\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt 0 | Step 2\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman'\n",
      "  Action token:      ',' (id: 11)\n",
      "  Policy logprob:    -3.7089\n",
      "  Reference logprob: -4.1556\n",
      "  KL divergence:     0.4467\n",
      "  Value:             1.2671\n",
      "  KL-adjusted reward:-0.7276\n",
      "  TD error:          -0.4070\n",
      "  Advantage:         -5.6454\n",
      "  Reward-to-go:      -4.3783\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt 0 | Step 3\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman,'\n",
      "  Action token:      ' whose' (id: 6693)\n",
      "  Policy logprob:    -3.7306\n",
      "  Reference logprob: -4.2126\n",
      "  KL divergence:     0.4820\n",
      "  Value:             1.6038\n",
      "  KL-adjusted reward:-0.7311\n",
      "  TD error:          -0.5590\n",
      "  Advantage:         -5.5699\n",
      "  Reward-to-go:      -3.9661\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt 0 | Step 4\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose'\n",
      "  Action token:      ' \"' (id: 330)\n",
      "  Policy logprob:    -5.3125\n",
      "  Reference logprob: -5.7155\n",
      "  KL divergence:     0.4030\n",
      "  Value:             1.7939\n",
      "  KL-adjusted reward:-0.7232\n",
      "  TD error:          -2.5203\n",
      "  Advantage:         -5.3279\n",
      "  Reward-to-go:      -3.5340\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt 0 | Step 5\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"'\n",
      "  Action token:      'other' (id: 1575)\n",
      "  Policy logprob:    -4.6505\n",
      "  Reference logprob: -5.4108\n",
      "  KL divergence:     0.7603\n",
      "  Value:             -0.0032\n",
      "  KL-adjusted reward:-0.7590\n",
      "  TD error:          0.1668\n",
      "  Advantage:         -2.9852\n",
      "  Reward-to-go:      -2.9884\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt 0 | Step 6\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"other'\n",
      "  Action token:      ' side' (id: 3108)\n",
      "  Policy logprob:    -1.6725\n",
      "  Reference logprob: -1.8130\n",
      "  KL divergence:     0.1405\n",
      "  Value:             0.9319\n",
      "  KL-adjusted reward:-0.6970\n",
      "  TD error:          -1.2223\n",
      "  Advantage:         -3.3514\n",
      "  Reward-to-go:      -2.4195\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt 0 | Step 7\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"other side'\n",
      "  Action token:      '\"' (id: 1)\n",
      "  Policy logprob:    -0.0169\n",
      "  Reference logprob: -0.0849\n",
      "  KL divergence:     0.0680\n",
      "  Value:             0.4107\n",
      "  KL-adjusted reward:-0.6897\n",
      "  TD error:          0.5263\n",
      "  Advantage:         -2.2638\n",
      "  Reward-to-go:      -1.8531\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt 0 | Step 8\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"other side\"'\n",
      "  Action token:      ' boyfriend' (id: 25838)\n",
      "  Policy logprob:    -5.5436\n",
      "  Reference logprob: -5.7867\n",
      "  KL divergence:     0.2431\n",
      "  Value:             1.6433\n",
      "  KL-adjusted reward:-0.7073\n",
      "  TD error:          -1.2004\n",
      "  Advantage:         -2.9667\n",
      "  Reward-to-go:      -1.3234\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt 0 | Step 9\n",
      "  State:             'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR: A woman, whose \"other side\" boyfriend'\n",
      "  Action token:      ' is' (id: 374)\n",
      "  Policy logprob:    -0.7388\n",
      "  Reference logprob: -1.0723\n",
      "  KL divergence:     0.3335\n",
      "  Value:             1.1618\n",
      "  KL-adjusted reward:-0.7163\n",
      "  TD error:          -1.8781\n",
      "  Advantage:         -1.8781\n",
      "  Reward-to-go:      -0.7163\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for pair in sa_pairs:\n",
    "    print(f\"Prompt {pair['prompt_idx']} | Step {pair['step']}\")\n",
    "    print(f\"  State:             {repr(pair['state_text'])}\")\n",
    "    print(f\"  Action token:      {repr(pair['action_token'])} (id: {pair['action_token_id']})\")\n",
    "    print(f\"  Policy logprob:    {pair['logprob']:.4f}\")\n",
    "    print(f\"  Reference logprob: {pair['ref_logprob']:.4f}\")\n",
    "    print(f\"  KL divergence:     {pair['kl_div']:.4f}\")\n",
    "    print(f\"  Value:             {pair['value']:.4f}\")\n",
    "    print(f\"  KL-adjusted reward:{pair['adjusted_reward']:.4f}\")\n",
    "    print(f\"  TD error:          {pair['td_error']:.4f}\")\n",
    "    print(f\"  Advantage:         {pair['advantage']:.4f}\")\n",
    "    print(f\"  Reward-to-go:      {pair['reward_to_go']:.4f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87c431cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value model training loss: 20.4032\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def train_value_model_one_step(\n",
    "    value_model, tokenizer, state_texts, reward_to_go_targets, device, optimizer, batch_size=32\n",
    "):\n",
    "    value_model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Prepare data\n",
    "\n",
    "\n",
    "    # Shuffle data for each epoch/step (optional)\n",
    "    indices = torch.randperm(len(state_texts))\n",
    "    state_texts = [state_texts[i] for i in indices]\n",
    "    reward_to_go_targets = [reward_to_go_targets[i] for i in indices]\n",
    "\n",
    "    for i in range(0, len(state_texts), batch_size):\n",
    "        batch_states = state_texts[i:i+batch_size]\n",
    "        batch_targets = torch.tensor(reward_to_go_targets[i:i+batch_size], dtype=torch.float32).to(device)\n",
    "\n",
    "        inputs = tokenizer(batch_states, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        preds = value_model(inputs[\"input_ids\"], inputs[\"attention_mask\"]).squeeze(-1)\n",
    "\n",
    "        loss = criterion(preds, batch_targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(batch_states)\n",
    "\n",
    "    avg_loss = total_loss / len(state_texts)\n",
    "    return avg_loss\n",
    "\n",
    "# Setup optimizer if not already done\n",
    "import torch.optim as optim\n",
    "value_model = value_model.to(device)\n",
    "value_optimizer = optim.AdamW(value_model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "state_texts = [pair['state_text'] for pair in sa_pairs]\n",
    "reward_to_go_targets = [pair['reward_to_go'] for pair in sa_pairs]\n",
    "# Training step\n",
    "avg_loss = train_value_model_one_step(\n",
    "    value_model, tokenizer, state_texts, reward_to_go_targets, device, value_optimizer, batch_size=32\n",
    ")\n",
    "print(f\"Value model training loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "884e9cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy model PPO loss: 3.5926\n"
     ]
    }
   ],
   "source": [
    "state_texts = [pair['state_text'] for pair in sa_pairs]\n",
    "action_token_ids = [pair['action_token_id'] for pair in sa_pairs]\n",
    "old_logprobs = torch.tensor([pair['logprob'] for pair in sa_pairs], dtype=torch.float32)\n",
    "advantages = torch.tensor([pair['advantage'] for pair in sa_pairs], dtype=torch.float32)\n",
    "\n",
    "def get_new_logprobs(policy_model, tokenizer, state_texts, action_token_ids, device, batch_size=32):\n",
    "    new_logprobs = []\n",
    "    for i in range(0, len(state_texts), batch_size):\n",
    "        batch_states = state_texts[i:i+batch_size]\n",
    "        batch_action_ids = action_token_ids[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_states, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        outputs = policy_model(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        logits = outputs.logits  # (batch, seq_len, vocab_size)\n",
    "        # Get logprobs for the last token in each sequence\n",
    "        log_probs = torch.nn.functional.log_softmax(logits[:, -1, :], dim=-1)  # (batch, vocab_size)\n",
    "        batch_action_ids_tensor = torch.tensor(batch_action_ids, dtype=torch.long, device=device)\n",
    "        batch_logprobs = log_probs[torch.arange(len(batch_states)), batch_action_ids_tensor]  # (batch,)\n",
    "        new_logprobs.append(batch_logprobs)\n",
    "    return torch.cat(new_logprobs, dim=0)  # (total_num_pairs,)\n",
    "\n",
    "def ppo_clip_loss(new_logprobs, old_logprobs, advantages, clip_epsilon=0.2):\n",
    "    ratio = torch.exp(new_logprobs - old_logprobs)\n",
    "    unclipped = ratio * advantages\n",
    "    clipped = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "    loss = -torch.mean(torch.min(unclipped, clipped))\n",
    "    return loss \n",
    "\n",
    "def train_policy_model_one_step(\n",
    "    policy_model, tokenizer, state_texts, action_token_ids, old_logprobs, advantages, device, optimizer, clip_epsilon=0.2, batch_size=32\n",
    "):\n",
    "    policy_model.train()\n",
    "    # Prepare data\n",
    "    state_texts = [pair['state_text'] for pair in sa_pairs]\n",
    "    action_token_ids = [pair['action_token_id'] for pair in sa_pairs]\n",
    "    old_logprobs = torch.tensor([pair['logprob'] for pair in sa_pairs], dtype=torch.float32).to(device)\n",
    "    advantages = torch.tensor([pair['advantage'] for pair in sa_pairs], dtype=torch.float32).to(device)\n",
    "\n",
    "    # Get new logprobs from current policy\n",
    "    new_logprobs = get_new_logprobs(policy_model, tokenizer, state_texts, action_token_ids, device, batch_size)\n",
    "\n",
    "    # Compute PPO-clip loss\n",
    "    loss = ppo_clip_loss(new_logprobs.to(device), old_logprobs, advantages, clip_epsilon)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "import torch.optim as optim\n",
    "policy_model = policy_model.to(device)\n",
    "policy_optimizer = optim.AdamW(policy_model.parameters(), lr=1e-5)\n",
    "\n",
    "loss = train_policy_model_one_step(\n",
    "    policy_model, tokenizer, state_texts, action_token_ids, old_logprobs, advantages, device, policy_optimizer, clip_epsilon=0.2, batch_size=32\n",
    ")\n",
    "print(f\"Policy model PPO loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "670b7012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[START] Prompt:\n",
      "['SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:']\n",
      "\n",
      "[AFTER GENERATION] sa_pairs[0]:\n",
      "{'state_text': 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:', 'action_token': ' I', 'action_token_id': 358, 'logprob': -1.483904242515564, 'ref_logprob': -1.9759289026260376, 'prompt_idx': 0, 'step': 0}\n",
      "\n",
      "Generated text:  I think not! I'm going to have to\n",
      "\n",
      "[BEFORE add_kl_to_sa_pairs] sa_pairs[0]:\n",
      "{'state_text': 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:', 'action_token': ' I', 'action_token_id': 358, 'logprob': -1.483904242515564, 'ref_logprob': -1.9759289026260376, 'prompt_idx': 0, 'step': 0}\n",
      "[AFTER add_kl_to_sa_pairs] sa_pairs[0]:\n",
      "{'state_text': 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:', 'action_token': ' I', 'action_token_id': 358, 'logprob': -1.483904242515564, 'ref_logprob': -1.9759289026260376, 'prompt_idx': 0, 'step': 0, 'kl_div': 0.49202466011047363}\n",
      "\n",
      "[BEFORE compute_rewards_for_sequences] generated_texts:\n",
      "[\" I think not! I'm going to have to\"]\n",
      "[AFTER compute_rewards_for_sequences] rewards:\n",
      "[-1.0883419513702393]\n",
      "\n",
      "[BEFORE adjust_rewards_with_kl] sa_pairs[0]:\n",
      "{'state_text': 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:', 'action_token': ' I', 'action_token_id': 358, 'logprob': -1.483904242515564, 'ref_logprob': -1.9759289026260376, 'prompt_idx': 0, 'step': 0, 'kl_div': 0.49202466011047363}\n",
      "[AFTER adjust_rewards_with_kl] sa_pairs[0]:\n",
      "{'state_text': 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:', 'action_token': ' I', 'action_token_id': 358, 'logprob': -1.483904242515564, 'ref_logprob': -1.9759289026260376, 'prompt_idx': 0, 'step': 0, 'kl_div': 0.49202466011047363, 'adjusted_reward': -1.1375444173812865}\n",
      "\n",
      "[BEFORE add_value_to_sa_pairs] sa_pairs[0]:\n",
      "{'state_text': 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:', 'action_token': ' I', 'action_token_id': 358, 'logprob': -1.483904242515564, 'ref_logprob': -1.9759289026260376, 'prompt_idx': 0, 'step': 0, 'kl_div': 0.49202466011047363, 'adjusted_reward': -1.1375444173812865}\n",
      "[AFTER add_value_to_sa_pairs] sa_pairs[0]:\n",
      "{'state_text': 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:', 'action_token': ' I', 'action_token_id': 358, 'logprob': -1.483904242515564, 'ref_logprob': -1.9759289026260376, 'prompt_idx': 0, 'step': 0, 'kl_div': 0.49202466011047363, 'adjusted_reward': -1.1375444173812865, 'value': 0.21869228780269623}\n",
      "\n",
      "[BEFORE compute_td_errors] sa_pairs[0]:\n",
      "{'state_text': 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:', 'action_token': ' I', 'action_token_id': 358, 'logprob': -1.483904242515564, 'ref_logprob': -1.9759289026260376, 'prompt_idx': 0, 'step': 0, 'kl_div': 0.49202466011047363, 'adjusted_reward': -1.1375444173812865, 'value': 0.21869228780269623}\n",
      "[AFTER compute_td_errors] sa_pairs[0]:\n",
      "{'state_text': 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:', 'action_token': ' I', 'action_token_id': 358, 'logprob': -1.483904242515564, 'ref_logprob': -1.9759289026260376, 'prompt_idx': 0, 'step': 0, 'kl_div': 0.49202466011047363, 'adjusted_reward': -1.1375444173812865, 'value': 0.21869228780269623, 'td_error': -4.307758516669273}\n",
      "\n",
      "[BEFORE compute_gae_advantages] sa_pairs[0]:\n",
      "{'state_text': 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:', 'action_token': ' I', 'action_token_id': 358, 'logprob': -1.483904242515564, 'ref_logprob': -1.9759289026260376, 'prompt_idx': 0, 'step': 0, 'kl_div': 0.49202466011047363, 'adjusted_reward': -1.1375444173812865, 'value': 0.21869228780269623, 'td_error': -4.307758516669273}\n",
      "[AFTER compute_gae_advantages] sa_pairs[0]:\n",
      "{'state_text': 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:', 'action_token': ' I', 'action_token_id': 358, 'logprob': -1.483904242515564, 'ref_logprob': -1.9759289026260376, 'prompt_idx': 0, 'step': 0, 'kl_div': 0.49202466011047363, 'adjusted_reward': -1.1375444173812865, 'value': 0.21869228780269623, 'td_error': -4.307758516669273, 'advantage': -9.181415043384664}\n",
      "\n",
      "[BEFORE add_reward_to_go_to_sa_pairs] sa_pairs[0]:\n",
      "{'state_text': 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:', 'action_token': ' I', 'action_token_id': 358, 'logprob': -1.483904242515564, 'ref_logprob': -1.9759289026260376, 'prompt_idx': 0, 'step': 0, 'kl_div': 0.49202466011047363, 'adjusted_reward': -1.1375444173812865, 'value': 0.21869228780269623, 'td_error': -4.307758516669273, 'advantage': -9.181415043384664}\n",
      "[AFTER add_reward_to_go_to_sa_pairs] sa_pairs[0]:\n",
      "{'state_text': 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:', 'action_token': ' I', 'action_token_id': 358, 'logprob': -1.483904242515564, 'ref_logprob': -1.9759289026260376, 'prompt_idx': 0, 'step': 0, 'kl_div': 0.49202466011047363, 'adjusted_reward': -1.1375444173812865, 'value': 0.21869228780269623, 'td_error': -4.307758516669273, 'advantage': -9.181415043384664, 'reward_to_go': -8.962722755581968}\n",
      "\n",
      "[BEFORE train_value_model_one_step] value model params (first layer):\n",
      "tensor([-0.0031,  0.0327, -0.0703, -0.0197, -0.0057])\n",
      "[AFTER train_value_model_one_step] value model params (first layer):\n",
      "tensor([-0.0031,  0.0327, -0.0703, -0.0197, -0.0057])\n",
      "Value model training loss: 26.1439266204834\n",
      "\n",
      "[BEFORE train_policy_model_one_step] policy model params (first layer):\n",
      "tensor([-0.0031,  0.0327, -0.0703, -0.0197, -0.0057])\n",
      "[AFTER train_policy_model_one_step] policy model params (first layer):\n",
      "tensor([-0.0031,  0.0327, -0.0703, -0.0197, -0.0057])\n",
      "Policy model PPO loss: 3.6040749549865723\n"
     ]
    }
   ],
   "source": [
    "# 1. Start with a prompt\n",
    "prompt = \"SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:\"\n",
    "prompts = [prompt]\n",
    "\n",
    "print(\"\\n[START] Prompt:\")\n",
    "print(prompts)\n",
    "\n",
    "# 2. Generate (s, a) pairs and logprobs from both policy and reference\n",
    "sa_pairs, generated_texts = generate_token_level_sa_and_logprobs_with_ref(\n",
    "    policy_model, sft_model, tokenizer, prompts, device, max_new_tokens=10\n",
    ")\n",
    "print(\"\\n[AFTER GENERATION] sa_pairs[0]:\")\n",
    "print(sa_pairs[0])\n",
    "print(\"\\nGenerated text:\", generated_texts[0])\n",
    "\n",
    "# 3. Add KL divergence to sa_pairs (if not already present)\n",
    "print(\"\\n[BEFORE add_kl_to_sa_pairs] sa_pairs[0]:\")\n",
    "print(sa_pairs[0])\n",
    "sa_pairs = add_kl_to_sa_pairs(sa_pairs)\n",
    "print(\"[AFTER add_kl_to_sa_pairs] sa_pairs[0]:\")\n",
    "print(sa_pairs[0])\n",
    "\n",
    "# 4. Compute reward for the generated sequence\n",
    "print(\"\\n[BEFORE compute_rewards_for_sequences] generated_texts:\")\n",
    "print(generated_texts)\n",
    "rewards = compute_rewards_for_sequences(reward_model, tokenizer, prompts, generated_texts, device)\n",
    "print(\"[AFTER compute_rewards_for_sequences] rewards:\")\n",
    "print(rewards)\n",
    "\n",
    "# 5. Add KL-adjusted reward to sa_pairs\n",
    "print(\"\\n[BEFORE adjust_rewards_with_kl] sa_pairs[0]:\")\n",
    "print(sa_pairs[0])\n",
    "sa_pairs = adjust_rewards_with_kl(sa_pairs, rewards, kl_coef=0.1)\n",
    "print(\"[AFTER adjust_rewards_with_kl] sa_pairs[0]:\")\n",
    "print(sa_pairs[0])\n",
    "\n",
    "# 6. Add value predictions to sa_pairs\n",
    "print(\"\\n[BEFORE add_value_to_sa_pairs] sa_pairs[0]:\")\n",
    "print(sa_pairs[0])\n",
    "sa_pairs = add_value_to_sa_pairs(sa_pairs, value_model, tokenizer, device, batch_size=32)\n",
    "print(\"[AFTER add_value_to_sa_pairs] sa_pairs[0]:\")\n",
    "print(sa_pairs[0])\n",
    "\n",
    "# 7. Compute TD errors\n",
    "print(\"\\n[BEFORE compute_td_errors] sa_pairs[0]:\")\n",
    "print(sa_pairs[0])\n",
    "sa_pairs = compute_td_errors(sa_pairs, gamma=0.99)\n",
    "print(\"[AFTER compute_td_errors] sa_pairs[0]:\")\n",
    "print(sa_pairs[0])\n",
    "\n",
    "# 8. Compute GAE advantages\n",
    "print(\"\\n[BEFORE compute_gae_advantages] sa_pairs[0]:\")\n",
    "print(sa_pairs[0])\n",
    "sa_pairs = compute_gae_advantages(sa_pairs, gamma=0.99, lam=0.95)\n",
    "print(\"[AFTER compute_gae_advantages] sa_pairs[0]:\")\n",
    "print(sa_pairs[0])\n",
    "\n",
    "# 9. Compute reward-to-go\n",
    "print(\"\\n[BEFORE add_reward_to_go_to_sa_pairs] sa_pairs[0]:\")\n",
    "print(sa_pairs[0])\n",
    "sa_pairs = add_reward_to_go_to_sa_pairs(sa_pairs)\n",
    "print(\"[AFTER add_reward_to_go_to_sa_pairs] sa_pairs[0]:\")\n",
    "print(sa_pairs[0])\n",
    "\n",
    "# 10. Train value model\n",
    "print(\"\\n[BEFORE train_value_model_one_step] value model params (first layer):\")\n",
    "print(list(value_model.parameters())[0][0][:5])  # print a few params\n",
    "state_texts = [pair['state_text'] for pair in sa_pairs]\n",
    "reward_to_go_targets = [pair['reward_to_go'] for pair in sa_pairs]\n",
    "value_optimizer = torch.optim.AdamW(value_model.parameters(), lr=1e-4)\n",
    "avg_value_loss = train_value_model_one_step(\n",
    "    value_model, tokenizer, state_texts, reward_to_go_targets, device, value_optimizer, batch_size=32\n",
    ")\n",
    "print(\"[AFTER train_value_model_one_step] value model params (first layer):\")\n",
    "print(list(value_model.parameters())[0][0][:5])\n",
    "print(\"Value model training loss:\", avg_value_loss)\n",
    "\n",
    "# 11. Train policy model (PPO-clip)\n",
    "print(\"\\n[BEFORE train_policy_model_one_step] policy model params (first layer):\")\n",
    "print(list(policy_model.parameters())[0][0][:5])\n",
    "state_texts = [pair['state_text'] for pair in sa_pairs]\n",
    "action_token_ids = [pair['action_token_id'] for pair in sa_pairs]\n",
    "old_logprobs = torch.tensor([pair['logprob'] for pair in sa_pairs], dtype=torch.float32).to(device)\n",
    "advantages = torch.tensor([pair['advantage'] for pair in sa_pairs], dtype=torch.float32).to(device)\n",
    "policy_optimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-5)\n",
    "avg_policy_loss = train_policy_model_one_step(\n",
    "    policy_model, tokenizer, state_texts, action_token_ids, old_logprobs, advantages, device, policy_optimizer, clip_epsilon=0.2, batch_size=32\n",
    ")\n",
    "print(\"[AFTER train_policy_model_one_step] policy model params (first layer):\")\n",
    "print(list(policy_model.parameters())[0][0][:5])\n",
    "print(\"Policy model PPO loss:\", avg_policy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d1e83398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: Input Prompt\n",
      "--------------------------------------------------------------------------------\n",
      "SUBREDDIT: r/relationships\n",
      "TITLE: Should I admit to snooping?\n",
      "POST: ...\n",
      "TL;DR:\n",
      "\n",
      "================================================================================\n",
      "STEP 2: Generate (state, action) pairs and logprobs\n",
      "Generated summary:  Should you or should it?\n",
      "The topic of this\n",
      "First (s, a) pair:\n",
      "{'state_text': 'SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:', 'action_token': ' Should', 'action_token_id': 12260, 'logprob': -2.2864532470703125, 'ref_logprob': -2.791689872741699, 'prompt_idx': 0, 'step': 0}\n",
      "\n",
      "================================================================================\n",
      "STEP 3: Add KL divergence to each (s, a) pair\n",
      "KL divergence for first pair: 0.5052\n",
      "\n",
      "================================================================================\n",
      "STEP 4: Compute reward for the generated sequence\n",
      "Reward for generated summary: 0.0146\n",
      "\n",
      "================================================================================\n",
      "STEP 5: Add KL-adjusted reward to each (s, a) pair\n",
      "KL-adjusted reward for first pair: -0.0359\n",
      "\n",
      "================================================================================\n",
      "STEP 6: Add value model predictions to each (s, a) pair\n",
      "Value prediction for first pair: -0.9528\n",
      "\n",
      "================================================================================\n",
      "STEP 7: Compute TD error for each (s, a) pair\n",
      "TD error for first pair: -0.5139\n",
      "\n",
      "================================================================================\n",
      "STEP 8: Compute GAE advantage for each (s, a) pair\n",
      "GAE advantage for first pair: 0.5659\n",
      "\n",
      "================================================================================\n",
      "STEP 9: Compute reward-to-go for each (s, a) pair\n",
      "Reward-to-go for first pair: -0.3869\n",
      "\n",
      "================================================================================\n",
      "STEP 10: Value model update (regress value to reward-to-go)\n",
      "Value model params (first 5): tensor([-0.0031,  0.0327, -0.0703, -0.0197, -0.0057])\n",
      "Value model params (first 5, after update): tensor([-0.0031,  0.0327, -0.0703, -0.0197, -0.0057])\n",
      "Value model training loss: 1.4987\n",
      "\n",
      "================================================================================\n",
      "STEP 11: Policy model PPO update (maximize clipped advantage)\n",
      "Policy model params (first 5): tensor([-0.0031,  0.0327, -0.0703, -0.0197, -0.0057])\n",
      "Policy model params (first 5, after update): tensor([-0.0031,  0.0327, -0.0703, -0.0197, -0.0057])\n",
      "Policy model PPO loss: 0.1915\n",
      "\n",
      "================================================================================\n",
      "SUMMARY: PPO step complete!\n",
      "Generated summary:  Should you or should it?\n",
      "The topic of this\n",
      "Final reward: 0.0146\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Start with a prompt\n",
    "prompt = \"SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: ...\\nTL;DR:\"\n",
    "prompts = [prompt]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: Input Prompt\")\n",
    "print(\"-\"*80)\n",
    "print(prompts[0])\n",
    "\n",
    "# 2. Generate (s, a) pairs and logprobs from both policy and reference\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Generate (state, action) pairs and logprobs\")\n",
    "sa_pairs, generated_texts = generate_token_level_sa_and_logprobs_with_ref(\n",
    "    policy_model, sft_model, tokenizer, prompts, device, max_new_tokens=10\n",
    ")\n",
    "print(f\"Generated summary: {generated_texts[0]}\")\n",
    "print(f\"First (s, a) pair:\\n{sa_pairs[0]}\")\n",
    "\n",
    "# 3. Add KL divergence to sa_pairs (if not already present)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: Add KL divergence to each (s, a) pair\")\n",
    "sa_pairs = add_kl_to_sa_pairs(sa_pairs)\n",
    "print(f\"KL divergence for first pair: {sa_pairs[0]['kl_div']:.4f}\")\n",
    "\n",
    "# 4. Compute reward for the generated sequence\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: Compute reward for the generated sequence\")\n",
    "rewards = compute_rewards_for_sequences(reward_model, tokenizer, prompts, generated_texts, device)\n",
    "print(f\"Reward for generated summary: {rewards[0]:.4f}\")\n",
    "\n",
    "# 5. Add KL-adjusted reward to sa_pairs\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: Add KL-adjusted reward to each (s, a) pair\")\n",
    "sa_pairs = adjust_rewards_with_kl(sa_pairs, rewards, kl_coef=0.1)\n",
    "print(f\"KL-adjusted reward for first pair: {sa_pairs[0]['adjusted_reward']:.4f}\")\n",
    "\n",
    "# 6. Add value predictions to sa_pairs\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: Add value model predictions to each (s, a) pair\")\n",
    "sa_pairs = add_value_to_sa_pairs(sa_pairs, value_model, tokenizer, device, batch_size=32)\n",
    "print(f\"Value prediction for first pair: {sa_pairs[0]['value']:.4f}\")\n",
    "\n",
    "# 7. Compute TD errors\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 7: Compute TD error for each (s, a) pair\")\n",
    "sa_pairs = compute_td_errors(sa_pairs, gamma=0.99)\n",
    "print(f\"TD error for first pair: {sa_pairs[0]['td_error']:.4f}\")\n",
    "\n",
    "# 8. Compute GAE advantages\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 8: Compute GAE advantage for each (s, a) pair\")\n",
    "sa_pairs = compute_gae_advantages(sa_pairs, gamma=0.99, lam=0.95)\n",
    "print(f\"GAE advantage for first pair: {sa_pairs[0]['advantage']:.4f}\")\n",
    "\n",
    "# 9. Compute reward-to-go\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 9: Compute reward-to-go for each (s, a) pair\")\n",
    "sa_pairs = add_reward_to_go_to_sa_pairs(sa_pairs)\n",
    "print(f\"Reward-to-go for first pair: {sa_pairs[0]['reward_to_go']:.4f}\")\n",
    "\n",
    "# 10. Train value model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 10: Value model update (regress value to reward-to-go)\")\n",
    "state_texts = [pair['state_text'] for pair in sa_pairs]\n",
    "reward_to_go_targets = [pair['reward_to_go'] for pair in sa_pairs]\n",
    "value_optimizer = torch.optim.AdamW(value_model.parameters(), lr=1e-4)\n",
    "print(\"Value model params (first 5):\", list(value_model.parameters())[0][0][:5])\n",
    "avg_value_loss = train_value_model_one_step(\n",
    "    value_model, tokenizer, state_texts, reward_to_go_targets, device, value_optimizer, batch_size=32\n",
    ")\n",
    "print(\"Value model params (first 5, after update):\", list(value_model.parameters())[0][0][:5])\n",
    "print(f\"Value model training loss: {avg_value_loss:.4f}\")\n",
    "\n",
    "# 11. Train policy model (PPO-clip)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 11: Policy model PPO update (maximize clipped advantage)\")\n",
    "state_texts = [pair['state_text'] for pair in sa_pairs]\n",
    "action_token_ids = [pair['action_token_id'] for pair in sa_pairs]\n",
    "old_logprobs = torch.tensor([pair['logprob'] for pair in sa_pairs], dtype=torch.float32).to(device)\n",
    "advantages = torch.tensor([pair['advantage'] for pair in sa_pairs], dtype=torch.float32).to(device)\n",
    "policy_optimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-5)\n",
    "print(\"Policy model params (first 5):\", list(policy_model.parameters())[0][0][:5])\n",
    "avg_policy_loss = train_policy_model_one_step(\n",
    "    policy_model, tokenizer, state_texts, action_token_ids, old_logprobs, advantages, device, policy_optimizer, clip_epsilon=0.2, batch_size=32\n",
    ")\n",
    "print(\"Policy model params (first 5, after update):\", list(policy_model.parameters())[0][0][:5])\n",
    "print(f\"Policy model PPO loss: {avg_policy_loss:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: PPO step complete!\")\n",
    "print(f\"Generated summary: {generated_texts[0]}\")\n",
    "print(f\"Final reward: {rewards[0]:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f0d88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'label'],\n",
      "        num_rows: 116722\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'label'],\n",
      "        num_rows: 6553\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['prompt', 'label'],\n",
      "        num_rows: 6447\n",
      "    })\n",
      "})\n",
      "SUBREDDIT: r/relationships\n",
      "TITLE: I (f/22) have to figure out if I want to still know these girls or not and would hate to sound insulting\n",
      "POST: Not sure if this belongs here but it's worth a try. \n",
      "\n",
      "Backstory:\n",
      "When I (f/22) went through my first real breakup 2 years ago because he needed space after a year of dating roand  it effected me more than I thought. It was a horrible time in my life due to living with my mother and finally having the chance to cut her out of my life. I can admit because of it was an emotional wreck and this guy was stable and didn't know how to deal with me. We ended by him avoiding for a month or so after going to a festival with my friends. When I think back I wish he just ended. So after he ended it added my depression I suffered but my friends helped me through it and I got rid of everything from him along with cutting contact. \n",
      "\n",
      "Now: Its been almost 3 years now and I've gotten better after counselling and mild anti depressants. My mother has been out of my life since then so there's been alot of progress. Being stronger after learning some lessons there been more insight about that time of my life but when I see him or a picture everything comes back. The emotions and memories bring me back down. \n",
      "\n",
      "His friends (both girls) are on my facebook because we get along well which is hard to find and I know they'll always have his back. But seeing him in a picture or talking to him at a convention having a conversation is tough. Crying confront of my current boyfriend is something I want to avoid. \n",
      "\n",
      "So I've been thinking that I have to cut contact with these girls because it's time to move on because it's healthier. It's best to avoid him as well. But will they be insulted? Will they accept it? Is there going to be awkwardness? I'm not sure if it's the right to do and could use some outside opinions.\n",
      "TL;DR: \n",
      "I still have contact with an old ex's friends but can't stand to see or talk to him. His friends are really nice ,so how do I tell them I possibly want to unfriend them on Facebook because of him?\n",
      "----------------------------------------\n",
      "SUBREDDIT: r/loseit\n",
      "TITLE: SV & NSV! Keeping on keeping on.\n",
      "POST: 30F, 5'6\". SW: 236 GW: 150 CW: 219\n",
      "\n",
      "I weigh myself weekly and measure myself monthly. I'd hit a plateau the last four weeks or so where I was stuck at 222. Felt like kind of a bummer, but knew it's because I haven't been as strict as I should with my diet, and the last week and a half have been crazy with life things, so I haven't been exercising as frequently as I've gotten used to. When I weighed myself as normal on Monday, I was kind of disappointed to see the scale not budging and figured it was time to buckle down again and really watch my diet. Today was my measure-in day, and I've felt cruddy in general since Monday because I caught some chest congestion/cold bug over the weekend. I get on the scale...it says 219. Whaaaaat? I take my measurements, which are down slightly from last month, and with an total-body loss of 8 inches from my starting point on 12/23/14! Some of my clothes have been feeling a bit looser as of late and now I know it's just not in my head. I'm now the lightest and smallest I've been since right around high school!\n",
      "TL;DR: \n",
      "Progress is still happening, even when you think it might not be! Don't get discouraged, even if your journey seems to be going slowly. Don't give up, warriors.\n",
      "----------------------------------------\n",
      "SUBREDDIT: r/relationships\n",
      "TITLE: Me [19F] with my friend [19M] 10 months, Insecurities - Show or Tell?\n",
      "POST: What are your stories about insecurities you've had in past relationships? How have you dealt with them, particularly the ones that you can't hide?\n",
      "\n",
      "I'm not currently in a relationship, but recently I've realized that there is someone who likes me, and I'm interested in them, too. Frankly, the only reason I'm not asking them out is because I know that I have some insecurities that need to be worked through - particularly in the realm of body image. While I'm confident in the rest of my body, I've had terrible, awful acne both on my arms and breasts since I was very young. It's a special type with no complete cure, but doctors suggested that I keep my skin oiled until it goes away (dryness irritates it). Because of this it's not so much present anymore as large clusters of scars are.\n",
      "\n",
      "Would I warn someone about this upfront before anything sexual? Would I just let it surprise them when the clothes come off? Do I tell them \"Let's keep on my shirt for now\" while we do our business? \n",
      "\n",
      "Have you had experiences with anything similar? I want to hear how they went!\n",
      "TL;DR: \n",
      "My skin is scarred badly; what could I do/say about it that would gross my future partner out the least? What's your experience with body image issues?\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "policy_dataset = load_dataset(\"CarperAI/openai_summarize_tldr\")\n",
    "\n",
    "print(policy_dataset)\n",
    "\n",
    "policy_train_data = policy_dataset[\"train\"]\n",
    "\n",
    "for i in range(3):\n",
    "    print(policy_dataset[\"train\"][i][\"prompt\"])\n",
    "    print(policy_dataset[\"train\"][i][\"label\"])\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb677104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected'],\n",
      "        num_rows: 92534\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected'],\n",
      "        num_rows: 83629\n",
      "    })\n",
      "    valid1: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected'],\n",
      "        num_rows: 33082\n",
      "    })\n",
      "    valid2: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected'],\n",
      "        num_rows: 50715\n",
      "    })\n",
      "})\n",
      "SUBREDDIT: r/relationships\n",
      "TITLE: To admit or not to admit snooping...\n",
      "POST: I [25M] have snooped in the past and copped up to it to my gf [25F] of 6 years.  We talked it through.  It had been a year or two since the last time.  That's an issue I'm working on.\n",
      "\n",
      "Now she has a new close male work friend.  I won't go into details, but she hides things from me with him and does other things to make me a bit suspicious.  So...I snooped again, and this time, all texts from her new friend have been deleted and I saw a google search for \"how to get over a guy\" near some searches of his name and views of his Facebook profile.\n",
      "\n",
      "I asked her about this guy, not mentioning the snooping, and she denied any feelings, we talked for a long time about our relationship and she insisted that she only loves me and I mean the world to her, and that she really wants to work towards getting this relationship back out of the rut we've been in (we both work all the time and barely see each other).\n",
      "\n",
      "I think if I cop to the snooping, we might have a more honest conversation about what's actually going on (if something is) and why she's having these feelings so we can either work through it together (my preference) or move on.  But obviously, it will open the pandora's box of the snooping.\n",
      "\n",
      "Think it's worth it to admit to the snooping to hopefully get to the bottom of this?\n",
      "TL;DR:  Snooped, found something, should I admit what I found so we can have a more honest conversation about it with less denial on her part?\n",
      "TL;DR:  I snooped, we talked about it, she wants to work it out, I'm not sure.  Is the snooping worth it?\n",
      "----------------------------------------\n",
      "SUBREDDIT: r/relationships\n",
      "TITLE: To admit or not to admit snooping...\n",
      "POST: I [25M] have snooped in the past and copped up to it to my gf [25F] of 6 years.  We talked it through.  It had been a year or two since the last time.  That's an issue I'm working on.\n",
      "\n",
      "Now she has a new close male work friend.  I won't go into details, but she hides things from me with him and does other things to make me a bit suspicious.  So...I snooped again, and this time, all texts from her new friend have been deleted and I saw a google search for \"how to get over a guy\" near some searches of his name and views of his Facebook profile.\n",
      "\n",
      "I asked her about this guy, not mentioning the snooping, and she denied any feelings, we talked for a long time about our relationship and she insisted that she only loves me and I mean the world to her, and that she really wants to work towards getting this relationship back out of the rut we've been in (we both work all the time and barely see each other).\n",
      "\n",
      "I think if I cop to the snooping, we might have a more honest conversation about what's actually going on (if something is) and why she's having these feelings so we can either work through it together (my preference) or move on.  But obviously, it will open the pandora's box of the snooping.\n",
      "\n",
      "Think it's worth it to admit to the snooping to hopefully get to the bottom of this?\n",
      "TL;DR:  I snooped, gf has new male friend, I'm not sure whether to confess snooping to try and get us back on track.\n",
      "TL;DR:  I snooped, we talked about it, she wants to work it out, I'm not sure.  Is the snooping worth it?\n",
      "----------------------------------------\n",
      "SUBREDDIT: r/relationships\n",
      "TITLE: To admit or not to admit snooping...\n",
      "POST: I [25M] have snooped in the past and copped up to it to my gf [25F] of 6 years.  We talked it through.  It had been a year or two since the last time.  That's an issue I'm working on.\n",
      "\n",
      "Now she has a new close male work friend.  I won't go into details, but she hides things from me with him and does other things to make me a bit suspicious.  So...I snooped again, and this time, all texts from her new friend have been deleted and I saw a google search for \"how to get over a guy\" near some searches of his name and views of his Facebook profile.\n",
      "\n",
      "I asked her about this guy, not mentioning the snooping, and she denied any feelings, we talked for a long time about our relationship and she insisted that she only loves me and I mean the world to her, and that she really wants to work towards getting this relationship back out of the rut we've been in (we both work all the time and barely see each other).\n",
      "\n",
      "I think if I cop to the snooping, we might have a more honest conversation about what's actually going on (if something is) and why she's having these feelings so we can either work through it together (my preference) or move on.  But obviously, it will open the pandora's box of the snooping.\n",
      "\n",
      "Think it's worth it to admit to the snooping to hopefully get to the bottom of this?\n",
      "TL;DR:  Snooped, found something, should I admit what I found so we can have a more honest conversation about it with less denial on her part?\n",
      "TL;DR:  Opened up snooping and found a google search for a guy's name and views of his facebook profile.  Girlfriend denies any feelings for him and insists that she only loves me and I mean the world to her, I'm not sure if I should admit to snooping and get to the bottom of this?\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reward_dataset = load_dataset(\"CarperAI/openai_summarize_comparisons\")\n",
    "\n",
    "print(reward_dataset)\n",
    "\n",
    "reward_train_data = reward_dataset[\"train\"]\n",
    "\n",
    "for i in range(3):\n",
    "    print(reward_dataset[\"train\"][i][\"prompt\"])\n",
    "    print(reward_dataset[\"train\"][i][\"chosen\"])\n",
    "    print(reward_dataset[\"train\"][i][\"rejected\"])\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98428a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 116722/116722 [01:55<00:00, 1013.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(example):\n",
    "    # You can add a prompt if you want, e.g., \"Summarize: \"\n",
    "    input_text = example[\"prompt\"]\n",
    "    target_text = example[\"label\"]\n",
    "    model_inputs = tokenizer(\n",
    "        input_text, max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        target_text, max_length=64, truncation=True, padding=\"max_length\"\n",
    "    )[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = policy_dataset[\"train\"].map(preprocess_function, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd893665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee784f22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m dataloader = DataLoader(dataset, batch_size=\u001b[32m1\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn=collate_fn)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Optimizer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m optimizer = torch.optim.AdamW(\u001b[43mmodel\u001b[49m.parameters(), lr=\u001b[32m5e-5\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[32m     36\u001b[39m model.train()\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare a small dataset for demonstration\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"CarperAI/openai_summarize_tldr\", split=\"train[:100]\")  # Use a small subset for speed\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Concatenate prompt and label for each example\n",
    "    input_texts = [x[\"prompt\"] + \"\\nTL;DR: \" + x[\"label\"] for x in batch]\n",
    "    # Tokenize the concatenated text\n",
    "    encodings = tokenizer(\n",
    "        input_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "    # For SFT, labels are the same as input_ids\n",
    "    labels = encodings[\"input_ids\"].clone()\n",
    "    encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "    labels = labels.to(device)\n",
    "    return encodings, labels\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(1):  # 1 epoch for demo\n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch} Batch {batch_idx} Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f59c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "prompt = \"SUBREDDIT: r/relationships\\nTITLE: Should I admit to snooping?\\nPOST: I snooped on my girlfriend's phone and found some suspicious messages. Should I tell her about it or keep it to myself?\\nTL;DR:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=10,  # Only generate 1 new token\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Prompt:\\n\", prompt)\n",
    "print(\"\\nGenerated TL;DR:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ade494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, base_model_name, lora_config):\n",
    "        super().__init__()\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "        self.base_model = get_peft_model(base_model, lora_config)\n",
    "        hidden_size = self.base_model.config.hidden_size\n",
    "        self.value_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        last_hidden = outputs.hidden_states[-1]\n",
    "        last_token_idx = attention_mask.sum(dim=1) - 1\n",
    "        last_token_idx = last_token_idx.unsqueeze(1).unsqueeze(2).expand(-1, 1, last_hidden.size(-1))\n",
    "        last_hidden_state = last_hidden.gather(1, last_token_idx).squeeze(1)\n",
    "        reward = self.value_head(last_hidden_state).squeeze(-1)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardComparisonDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_length=512):\n",
    "        self.data = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        prompt = item[\"prompt\"]\n",
    "        chosen = item[\"chosen\"]\n",
    "        rejected = item[\"rejected\"]\n",
    "\n",
    "        def encode(text):\n",
    "            return self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        chosen_input = encode(prompt + chosen)\n",
    "        rejected_input = encode(prompt + rejected)\n",
    "\n",
    "        return {\n",
    "            \"input_ids_chosen\": chosen_input[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask_chosen\": chosen_input[\"attention_mask\"].squeeze(),\n",
    "            \"input_ids_rejected\": rejected_input[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask_rejected\": rejected_input[\"attention_mask\"].squeeze(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "def pairwise_loss(chosen_reward, rejected_reward):\n",
    "    return -torch.log(torch.sigmoid(chosen_reward - rejected_reward)).mean()\n",
    "\n",
    "reward_model = RewardModel(model_name, lora_config)\n",
    "reward_model.train()\n",
    "optimizer = torch.optim.AdamW(reward_model.parameters(), lr=5e-6)\n",
    "\n",
    "reward_dataset = RewardComparisonDataset(summarize_train_data, tokenizer)\n",
    "reward_loader = DataLoader(reward_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "for epoch in range(1):  # 1 epoch\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(reward_loader):\n",
    "        input_ids_chosen = batch[\"input_ids_chosen\"]\n",
    "        attention_mask_chosen = batch[\"attention_mask_chosen\"]\n",
    "        input_ids_rejected = batch[\"input_ids_rejected\"]\n",
    "        attention_mask_rejected = batch[\"attention_mask_rejected\"]\n",
    "\n",
    "        chosen_reward = reward_model(input_ids_chosen, attention_mask_chosen)\n",
    "        rejected_reward = reward_model(input_ids_rejected, attention_mask_rejected)\n",
    "\n",
    "        loss = pairwise_loss(chosen_reward, rejected_reward)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Avg Loss: {total_loss / len(reward_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14497ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer, PPOConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Prepare your policy and reward models (already done)\n",
    "# 2. Prepare your tokenizer (already done)\n",
    "\n",
    "# 3. PPO config\n",
    "ppo_config = PPOConfig(\n",
    "    batch_size=4,\n",
    "    forward_batch_size=2,\n",
    "    learning_rate=5e-6,\n",
    "    log_with=None,  # or \"wandb\"\n",
    "    mini_batch_size=4,\n",
    "    optimize_cuda_cache=True,\n",
    ")\n",
    "\n",
    "# 4. Prepare your dataset (list of prompts)\n",
    "prompts = [item[\"prompt\"] for item in tldr_train_data]\n",
    "\n",
    "# 5. PPOTrainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=summariser_model,\n",
    "    ref_model=None,  # Optional: a reference model for KL penalty\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=prompts,\n",
    "    reward_model=reward_model,\n",
    ")\n",
    "\n",
    "# 6. Run PPO training\n",
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8358e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PPO\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
