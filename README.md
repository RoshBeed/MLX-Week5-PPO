# PPO Language Model Fine-tuning

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PPO](https://img.shields.io/badge/PPO-Implemented-orange.svg)](https://arxiv.org/abs/1707.06347)

A production-ready implementation of Proximal Policy Optimization (PPO) for language model fine-tuning. This project provides a modular, scalable architecture for training language models with human feedback through reinforcement learning.

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd MLX-Week5-PPO

# Install dependencies using uv
uv sync
```

### Basic Usage

```python
from config import PPOConfig
from ppo_trainer import PPOTrainer

# Initialize configuration
config = PPOConfig()

# Create trainer
trainer = PPOTrainer(config)

# Example prompts
prompts = [
    "SUBREDDIT: r/relationships\nTITLE: Should I admit to snooping?\nPOST: ...\nTL;DR:",
    "SUBREDDIT: r/advice\nTITLE: Need help with decision\nPOST: ...\nTL;DR:"
]

# Run PPO step
results = trainer.ppo_step(prompts)

print(f"Generated texts: {results['generated_texts']}")
print(f"Rewards: {results['rewards']}")
print(f"Value loss: {results['value_loss']:.4f}")
print(f"Policy loss: {results['policy_loss']:.4f}")
```

## ğŸ“ Project Structure

```
MLX-Week5-PPO/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ ppo_config.py          # PPO configuration
â”œâ”€â”€ base_setup/
â”‚   â”œâ”€â”€ base_setup.py          # Base model setup
â”‚   â”œâ”€â”€ lora_setup.py          # LoRA configuration
â”‚   â”œâ”€â”€ model_setup.py         # Model initialization
â”‚   â””â”€â”€ tokenizer_setup.py     # Tokenizer setup
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base_model.py          # Base model class
â”‚   â”œâ”€â”€ policy_model.py        # Policy model
â”‚   â”œâ”€â”€ reward_model.py        # Reward model
â”‚   â”œâ”€â”€ sft_model.py           # SFT model
â”‚   â””â”€â”€ value_model.py         # Value model
â”œâ”€â”€ ppo_trainer/
â”‚   â”œâ”€â”€ ppo_trainer.py         # Main PPO trainer
â”‚   â”œâ”€â”€ generator.py           # Sequence generator
â”‚   â”œâ”€â”€ reward_computer.py     # Reward computation
â”‚   â”œâ”€â”€ value_adder.py         # Value addition
â”‚   â””â”€â”€ model_trainer.py       # Model training
â”œâ”€â”€ token_attribution/
â”‚   â”œâ”€â”€ token_attribution.py   # Token attribution
â”‚   â”œâ”€â”€ extractor.py           # Feature extraction
â”‚   â””â”€â”€ sa_pair_builder.py     # State-action pair builder
â”œâ”€â”€ reward_kl/
â”‚   â”œâ”€â”€ kl_divergence.py       # KL divergence computation
â”‚   â””â”€â”€ reward_adjuster.py     # Reward adjustment
â”œâ”€â”€ advantage/
â”‚   â”œâ”€â”€ gae_advantage.py       # GAE advantage computation
â”‚   â”œâ”€â”€ return_calculator.py   # Return calculation
â”‚   â””â”€â”€ td_error.py            # TD error computation
â”œâ”€â”€ experience_buffer/
â”‚   â””â”€â”€ buffer.py              # Experience buffer
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ policy_loss.py         # Policy loss computation
â”‚   â”œâ”€â”€ policy_ratio.py        # Policy ratio computation
â”‚   â””â”€â”€ value_loss.py          # Value loss computation
â”œâ”€â”€ main.py                    # Main execution script
â””â”€â”€ pyproject.toml            # Project configuration
```

## ğŸ”§ Configuration

The `PPOConfig` class in `config/ppo_config.py` contains all configurable parameters:

```python
@dataclass
class PPOConfig:
    # Model configs
    model_name: str = "Qwen/Qwen3-0.6B-Base"
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    
    # Training configs
    learning_rate: float = 1e-5
    value_learning_rate: float = 1e-4
    batch_size: int = 32
    max_new_tokens: int = 10
    gamma: float = 0.99
    lam: float = 0.95
    kl_coef: float = 0.1
    clip_epsilon: float = 0.2
    
    # Generation configs
    temperature: float = 1.0
    top_k: int = 50
    top_p: float = 0.95
```

## ğŸ—ï¸ Architecture

### Core Components

#### PPOTrainer
The main orchestrator that coordinates all PPO components and executes the training loop.

**Key Methods:**
- `__init__(config)`: Initializes all models and components
- `ppo_step(prompts)`: Executes one complete PPO training step

**Responsibilities:**
- Orchestrates the entire PPO training process
- Manages model initialization and device placement
- Coordinates data flow between all components

#### Models

**PolicyModel**: The main model being trained with PPO
- Generates text sequences token by token
- Provides log probabilities for actions
- Updated using PPO policy loss

**SFTModel**: Supervised Fine-Tuned reference model
- Used to compute KL divergence with policy model
- Provides reference log probabilities for each action
- Remains frozen during training

**ValueModel**: Estimates state values
- Predicts expected future rewards for each state
- Trained to minimize MSE with reward-to-go targets
- Used for advantage computation

**RewardModel**: Computes rewards for generated sequences
- Evaluates the quality of generated text
- Returns scalar reward values
- Can be human feedback or automated scoring

#### SequenceGenerator
Handles text generation and state-action pair extraction.

**Key Methods:**
- `generate_sequences(prompts)`: Generates text and extracts (s,a) pairs

**Process:**
1. Generates text sequences using policy model
2. For each token, extracts:
   - State: prompt + previously generated tokens
   - Action: next token
   - Policy log probability
   - Reference log probability (from SFT model)

#### KLDivergence
Computes KL divergence between policy and reference models.

**Key Methods:**
- `add_kl_to_sa_pairs(sa_pairs)`: Adds KL divergence to each pair

**Process:**
- Calculates KL divergence per token: `logprob_policy - logprob_ref`
- Used for reward adjustment to prevent policy drift

#### RewardComputer
Computes rewards for generated sequences.

**Key Methods:**
- `compute_rewards(prompts, generated_texts)`: Returns reward values

**Process:**
- Takes full text (prompt + generated) as input
- Returns scalar reward values for each sequence

#### RewardAdjuster
Applies KL penalty to rewards to prevent policy drift.

**Process:**
- Adjusts rewards: `reward = reward - kl_coef * kl_divergence`
- Balances reward maximization with staying close to reference model

#### ValueAdder
Adds value predictions to state-action pairs.

**Key Methods:**
- `add_values_to_sa_pairs(sa_pairs)`: Adds value predictions

**Process:**
- Computes value predictions for each state
- Adds values to state-action pairs for advantage computation

#### TDError
Computes temporal difference errors.

**Process:**
- Calculates TD error: `reward + gamma * next_value - current_value`
- Used for advantage computation

#### GAEAdvantage
Computes Generalized Advantage Estimation.

**Key Methods:**
- `compute_gae_advantages(sa_pairs, gamma, lam)`: Returns GAE advantages

**Process:**
- Groups pairs by prompt sequence
- Computes GAE recursively: `gae = td_error + gamma * lam * next_gae`
- Provides stable advantage estimates

#### ReturnCalculator
Computes reward-to-go values.

**Process:**
- Calculates cumulative future rewards for each state
- Used as targets for value model training

#### ModelTrainer
Handles training of policy and value models.

**Key Methods:**
- `train_value_model(sa_pairs)`: Trains value model on reward-to-go targets
- `train_policy_model(sa_pairs)`: Trains policy model using PPO loss

**Process:**
- Value model: Minimizes MSE with reward-to-go targets
- Policy model: Maximizes PPO-clipped objective with advantages

### PPO Training Flow
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          PPO Training Pipeline                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚  â”‚    Input Data      â”‚                                                       â”‚
â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                                                       â”‚
â”‚  â”‚ â€¢ Prompts          â”‚                                                       â”‚
â”‚  â”‚ â€¢ Datasets         â”‚                                                       â”‚
â”‚  â”‚ â€¢ Human Feedback   â”‚                                                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚         â”‚                                                                     â”‚
â”‚         â–¼                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                   Generation & Token Attribution                         â”‚ â”‚
â”‚  â”‚                                                                          â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚ â”‚
â”‚  â”‚  â”‚ Policy Model  â”‚ â”‚   SFT Model   â”‚ â”‚   Tokenizer   â”‚                   â”‚ â”‚
â”‚  â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                   â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ generate()  â”‚ â”‚ â€¢ Ref Logprobsâ”‚ â”‚ â€¢ Tokenize    â”‚                   â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Logprobs    â”‚ â”‚ â€¢ KL Baseline â”‚ â”‚ â€¢ Decode      â”‚                   â”‚ â”‚
â”‚  â”‚  â”‚               â”‚ â”‚               â”‚ â”‚ â€¢ SA Extract  â”‚                   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚ â”‚
â”‚  â”‚         â”‚                  â”‚                    â”‚                        â”‚ â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚ â”‚
â”‚  â”‚                            â–¼                                             â”‚ â”‚
â”‚  â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚ â”‚
â”‚  â”‚             â”‚    generate_sequences()    â”‚                               â”‚ â”‚
â”‚  â”‚             â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                               â”‚ â”‚
â”‚  â”‚             â”‚ â€¢ Input: List[str]         â”‚                               â”‚ â”‚
â”‚  â”‚             â”‚ â€¢ Output: SA pairs, text   â”‚                               â”‚ â”‚
â”‚  â”‚             â””--------â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                                     â”‚                               â”‚
â”‚         â–¼                                     â–¼                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚   Reward Model     â”‚       â”‚               KL Divergence â”‚                 â”‚
â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                 â”‚
â”‚  â”‚ â€¢ compute_rewards()â”‚       â”‚ â€¢ add_kl_to_sa_pairs()      â”‚                 â”‚
â”‚  â”‚ â€¢ Output: floats   â”‚       â”‚ â€¢ Output: SA pairs w/ KL    â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚         â”‚                                     â”‚                               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                 â–¼                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”€â” â”‚
â”‚  â”‚                      Reward Adjustment Pipeline                          â”‚ â”‚
â”‚  â”‚                                                                          â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚ â”‚
â”‚  â”‚  â”‚  Adjuster     â”‚ â”‚ Enhanced SA   â”‚ â”‚ Value Model   â”‚                   â”‚ â”‚
â”‚  â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                   â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ adjust_kl() â”‚ â”‚ â€¢ Adjusted    â”‚ â”‚ â€¢ add_values()â”‚                   â”‚ â”‚
â”‚  â”‚  â”‚               â”‚ â”‚   Rewards     â”‚ â”‚               â”‚                   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚            â”‚                â”‚                    â”‚                            â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                             â–¼                                                 â”‚
â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚               â”‚  Advantage Computation       â”‚                                â”‚
â”‚               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                                â”‚
â”‚               â”‚ â€¢ TD Errors                  â”‚                                â”‚
â”‚               â”‚ â€¢ GAE Advantages             â”‚                                â”‚
â”‚               â”‚ â€¢ Reward-to-go               â”‚                                â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                            â”‚                                                  â”‚
â”‚                            â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                          Model Training                                  â”‚ â”‚
â”‚  â”‚                                                                          â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚ â”‚
â”‚  â”‚  â”‚ Value Trainer â”‚ â”‚ Policy Trainerâ”‚ â”‚ Experience Bufâ”‚                   â”‚ â”‚
â”‚  â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                   â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ MSE Loss    â”‚ â”‚ â€¢ PPO Loss    â”‚ â”‚ â€¢ SA Batching â”‚                   â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Updates     â”‚ â”‚ â€¢ Grad Step   â”‚ â”‚               â”‚                   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚ â”‚
â”‚  â”‚         â”‚                  â”‚                 â”‚                           â”‚ â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€----â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚ â”‚
â”‚  â”‚                            â–¼                                             â”‚ â”‚
â”‚  â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚ â”‚
â”‚  â”‚                   â”‚ Updated Models  â”‚                                    â”‚ â”‚
â”‚  â”‚                   â”‚â”€--â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                                    â”‚ â”‚
â”‚  â”‚                   â”‚ â€¢ Policy Model  â”‚                                    â”‚ â”‚
â”‚  â”‚                   â”‚ â€¢ Value Model   â”‚                                    â”‚ â”‚
â”‚  â”‚                   â”‚ â€¢ Ready for     â”‚                                    â”‚ â”‚
â”‚  â”‚                   â”‚   Next Step     â”‚                                    â”‚ â”‚
â”‚  â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ 
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Component Details:
==================

1. INPUT DATA LAYER
   - Prompts: Text inputs for generation
   - Datasets: Training and evaluation data
   - Human Feedback: Reward signals and preferences

2. GENERATION & TOKEN ATTRIBUTION LAYER
   - Policy Model: Current policy for text generation
   - SFT Model: Reference model for KL divergence
   - Tokenizer: Text tokenization and state-action extraction
   - generate_sequences(): Creates SA pairs and generated text

3. KL DIVERGENCE & REWARDS LAYER 
   - KL Divergence: Distance from reference model (add_kl_to_sa_pairs)
   - Reward Model: Learned reward function (compute_rewards)
   - Reward Adjuster: KL penalty application (adjust_rewards_with_kl)

4. ADVANTAGE & VALUE COMPUTATION LAYER
   - Value Model: State value predictions (add_values_to_sa_pairs)
   - TD Error: Temporal difference error computation (compute_td_errors)
   - GAE Advantage: Generalized advantage estimation (compute_gae_advantages)
   - Return Calculator: Reward-to-go computation (add_reward_to_go)

5. MODEL TRAINING LAYER
   - Value Model Trainer: MSE loss and value updates (train_value_model)
   - Policy Model Trainer: PPO-clip loss and policy updates (train_policy_model)
   - Experience Buffer: Training data storage


Data Flow:
==========

1. Prompts â†’ generate_sequences() â†’ State-Action Pairs + Generated Text
2. Generated Text â†’ compute_rewards() â†’ Rewards
3. State-Action Pairs â†’ add_kl_to_sa_pairs() â†’ Enhanced SA Pairs
4. Enhanced SA Pairs + Rewards â†’ adjust_rewards_with_kl() â†’ Adjusted SA Pairs
5. Adjusted SA Pairs â†’ add_values_to_sa_pairs() â†’ SA Pairs with Values
6. SA Pairs with Values â†’ compute_td_errors() + compute_gae_advantages() + add_reward_to_go() â†’ Training Targets
7. Training Targets â†’ train_value_model() + train_policy_model() â†’ Updated Models
8. Updated Models â†’ Next Generation Cycle (Feedback Loop)

This architecture enables efficient, stable, and scalable PPO training for language models
with human feedback, supporting both research and production deployments. 


## ğŸš€ Running the Project

```bash
# Run the main script
python main.py
```

## ğŸ“š Dependencies

The project uses `uv` for dependency management. Key dependencies include:

- `torch` - PyTorch for deep learning
- `transformers` - Hugging Face transformers library
- `peft` - Parameter-efficient fine-tuning
- `datasets` - Dataset handling
- `numpy` - Numerical computing
- `tqdm` - Progress bars

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.