# MLX PPO Language Model Fine-tuning

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PPO](https://img.shields.io/badge/PPO-Implemented-orange.svg)](https://arxiv.org/abs/1707.06347)

A production-ready implementation of Proximal Policy Optimization (PPO) for language model fine-tuning. This project provides a modular, scalable architecture for training language models with human feedback through reinforcement learning.

## ğŸš€ Quick Start

```bash
# Clone the repository
git clone https://github.com/RoshBeed/MLX-Week5-PPO.git
cd MLX-Week5-PPO

# Install dependencies
uv sync

# Run a quick demo
python main.py
```

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Installation](#installation)
- [Usage](#usage)
- [Configuration](#configuration)
- [API Reference](#api-reference)
- [Examples](#examples)
- [Contributing](#contributing)
- [License](#license)

## ğŸ¯ Overview

This project implements a complete PPO training pipeline for language models, featuring:

- **Modular Architecture**: Clean separation of concerns with dedicated components
- **Production Ready**: Comprehensive error handling, logging, and monitoring
- **Extensible Design**: Easy to add new models, reward functions, and training strategies
- **Research Friendly**: Supports experimentation with different PPO variants

### Key Features

- âœ… **Token-level PPO**: Fine-grained control over language generation
- âœ… **KL Divergence Penalty**: Prevents policy collapse
- âœ… **GAE Advantage Estimation**: Stable advantage computation
- âœ… **Value Function Learning**: Separate value network for better estimates
- âœ… **Experience Buffering**: Efficient memory management
- âœ… **Modular Components**: Pluggable architecture for easy customization

## ğŸ—ï¸ Architecture

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        PPO Training Pipeline                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   Prompt    â”‚â”€â”€â”€â–¶â”‚   Policy    â”‚â”€â”€â”€â–¶â”‚  Generated  â”‚          â”‚
â”‚  â”‚  Dataset    â”‚    â”‚   Model     â”‚    â”‚    Text     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                           â”‚                       â”‚             â”‚
â”‚                           â–¼                       â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   Reward    â”‚â—€â”€â”€â”€â”‚   Token     â”‚â—€â”€â”€â”€â”‚   Value     â”‚          â”‚
â”‚  â”‚   Model     â”‚    â”‚ Attribution â”‚    â”‚   Model     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚         â”‚                   â”‚                   â”‚               â”‚
â”‚         â–¼                   â–¼                   â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   Reward    â”‚    â”‚   KL &      â”‚    â”‚   Advantage â”‚          â”‚
â”‚  â”‚ Computation â”‚    â”‚   Penalty   â”‚    â”‚ Computation â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚         â”‚                   â”‚                   â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                             â–¼                                   â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚                    â”‚ Experience  â”‚                              â”‚
â”‚                    â”‚   Buffer    â”‚                              â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                             â”‚                                   â”‚
â”‚                             â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          |
â”‚  â”‚   Policy    â”‚â—€â”€â”€â”€â”‚   Model     â”‚â”€â”€â”€â–¶â”‚   Value     â”‚          |
â”‚  â”‚   Update    â”‚    â”‚  Training   â”‚    â”‚   Update    â”‚          |
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          |
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Architecture

```
config/
â”œâ”€â”€ ppo_config.py          # PPO hyperparameters and settings
â”œâ”€â”€ model_config.py        # Model architecture configurations
â””â”€â”€ training_config.py     # Training loop configurations

base_setup/
â”œâ”€â”€ tokenizer_setup.py     # Tokenizer initialization and management
â””â”€â”€ lora_setup.py          # LoRA adapter configuration

models/
â”œâ”€â”€ sft_model.py           # Supervised Fine-Tuned base model
â”œâ”€â”€ policy_model.py        # PPO policy network
â”œâ”€â”€ value_model.py         # Value function network
â””â”€â”€ reward_model.py        # Reward model for human feedback

token_attribution/
â”œâ”€â”€ extractor.py           # Token-level state-action extraction
â””â”€â”€ logprob_computer.py    # Log probability computation

reward_kl/
â”œâ”€â”€ reward_computer.py     # Reward signal computation
â””â”€â”€ kl_divergence.py       # KL divergence calculation

advantage/
â”œâ”€â”€ td_error.py            # Temporal Difference error computation
â”œâ”€â”€ gae_advantage.py       # Generalized Advantage Estimation
â””â”€â”€ reward_to_go.py        # Reward-to-go calculation

experience_buffer/
â”œâ”€â”€ buffer_manager.py      # Experience buffer management
â””â”€â”€ data_structures.py     # Buffer data structures

training/
â”œâ”€â”€ policy_loss.py         # PPO policy loss computation
â”œâ”€â”€ value_loss.py          # Value function loss
â””â”€â”€ model_trainer.py       # Training loop orchestration

ppo_trainer/
â”œâ”€â”€ ppo_trainer.py         # Main PPO training orchestrator
â”œâ”€â”€ generator.py           # Text generation utilities
â””â”€â”€ evaluator.py           # Model evaluation utilities
```

## ğŸ“¦ Installation

### Prerequisites

- Python 3.8 or higher
- 16GB+ RAM recommended
- 50GB+ free disk space for model storage

### Installation Steps

1. **Clone the repository**
   ```bash
   git clone https://github.com/RoshBeed/MLX-Week5-PPO.git
   cd MLX-Week5-PPO
   ```

2. **Create virtual environment**
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   uv sync
   ```

## ğŸš€ Usage

### Basic Usage

```python
from ppo_trainer.ppo_trainer import PPOTrainer
from config.ppo_config import PPOConfig

# Initialize configuration
config = PPOConfig(
    model_name="Qwen/Qwen3-0.6B-Base",
    max_new_tokens=50,
    batch_size=4,
    learning_rate=1e-5,
    kl_coef=0.1
)

# Create trainer
trainer = PPOTrainer(config)

# Train the model
trainer.train(
    prompts=["Your prompt here"],
    num_epochs=10
)
```

### Advanced Usage

```python
from ppo_trainer.ppo_trainer import PPOTrainer
from config.ppo_config import PPOConfig
from models.custom_reward_model import CustomRewardModel

# Custom configuration
config = PPOConfig(
    model_name="Qwen/Qwen3-0.6B-Base",
    max_new_tokens=100,
    batch_size=8,
    learning_rate=5e-6,
    kl_coef=0.2,
    gamma=0.99,
    lam=0.95,
    clip_epsilon=0.2
)

# Custom reward model
reward_model = CustomRewardModel()

# Initialize trainer with custom components
trainer = PPOTrainer(
    config=config,
    reward_model=reward_model
)

# Training with custom callbacks
def on_epoch_end(epoch, metrics):
    print(f"Epoch {epoch}: Reward = {metrics['avg_reward']:.4f}")

trainer.train(
    prompts=prompts,
    num_epochs=20,
    callbacks=[on_epoch_end]
)
```

## âš™ï¸ Configuration

### PPO Configuration

```python
@dataclass
class PPOConfig:
    # Model settings
    model_name: str = "Qwen/Qwen3-0.6B-Base"
    max_new_tokens: int = 50
    
    # Training hyperparameters
    batch_size: int = 4
    learning_rate: float = 1e-5
    kl_coef: float = 0.1
    gamma: float = 0.99
    lam: float = 0.95
    clip_epsilon: float = 0.2
    
    # LoRA settings
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.1
```

### Model Configuration

```python
@dataclass
class ModelConfig:
    # Architecture settings
    hidden_size: int = 768
    num_attention_heads: int = 12
    num_layers: int = 12
    
    # Training settings
    dropout: float = 0.1
    layer_norm_eps: float = 1e-5
```

## ğŸ“š API Reference

### PPOTrainer

Main training orchestrator for PPO fine-tuning.

```python
class PPOTrainer:
    def __init__(self, config: PPOConfig, **kwargs):
        """Initialize PPO trainer with configuration."""
        
    def train(self, prompts: List[str], num_epochs: int = 10) -> Dict:
        """Train the model using PPO."""
        
    def evaluate(self, prompts: List[str]) -> Dict:
        """Evaluate the trained model."""
        
    def save_model(self, path: str):
        """Save the trained model."""
        
    def load_model(self, path: str):
        """Load a trained model."""
```

### Key Components

#### Policy Model
```python
class PolicyModel:
    def forward(self, input_ids, attention_mask=None) -> ModelOutput:
        """Forward pass through policy network."""
        
    def generate(self, input_ids, **kwargs) -> torch.Tensor:
        """Generate text using current policy."""
```

#### Value Model
```python
class ValueModel:
    def forward(self, input_ids, attention_mask=None) -> torch.Tensor:
        """Predict value for given state."""
```

#### Reward Model
```python
class RewardModel:
    def forward(self, input_ids, attention_mask=None) -> torch.Tensor:
        """Compute reward for given text."""
```



## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **OpenAI** for the original PPO algorithm
- **Hugging Face** for the transformers library
- **Qwen Team** for the base language models

## ğŸ“ Support

- **Documentation**: [docs/](docs/)
- **Issues**: [GitHub Issues](https://github.com/RoshBeed/MLX-Week5-PPO/issues)
- **Discussions**: [GitHub Discussions](https://github.com/RoshBeed/MLX-Week5-PPO/discussions)
- **Email**: rosh.beed@roshbeed.com

---

**Made with â¤ï¸ by Rosh**
