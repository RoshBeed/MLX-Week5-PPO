# PPO Implementation for Language Models

A modular implementation of Proximal Policy Optimization (PPO) for language model fine-tuning, built with PyTorch and Transformers.

## üèóÔ∏è Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        PPO Implementation                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ   Config    ‚îÇ    ‚îÇ Base Setup  ‚îÇ    ‚îÇ   Models    ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ    ‚îÇ             ‚îÇ    ‚îÇ             ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ PPOConfig ‚îÇ    ‚îÇ ‚Ä¢ Tokenizer ‚îÇ    ‚îÇ ‚Ä¢ SFT Model ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Hyperparams‚îÇ   ‚îÇ ‚Ä¢ LoRA Setup‚îÇ    ‚îÇ ‚Ä¢ Policy    ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Training  ‚îÇ    ‚îÇ ‚Ä¢ Model Base‚îÇ    ‚îÇ ‚Ä¢ Reward    ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ   Settings  ‚îÇ    ‚îÇ             ‚îÇ    ‚îÇ ‚Ä¢ Value     ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ   Token     ‚îÇ    ‚îÇ  Reward/KL  ‚îÇ    ‚îÇ  Advantage  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇAttribution  ‚îÇ    ‚îÇ             ‚îÇ    ‚îÇ             ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ    ‚îÇ ‚Ä¢ KL Div    ‚îÇ    ‚îÇ ‚Ä¢ TD Error  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Extractor ‚îÇ    ‚îÇ ‚Ä¢ Reward    ‚îÇ    ‚îÇ ‚Ä¢ GAE       ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ SA Builder‚îÇ    ‚îÇ   Adjuster  ‚îÇ    ‚îÇ ‚Ä¢ Returns   ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ Experience  ‚îÇ    ‚îÇ  Training   ‚îÇ    ‚îÇ PPO Trainer ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ   Buffer    ‚îÇ    ‚îÇ             ‚îÇ    ‚îÇ             ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ    ‚îÇ ‚Ä¢ Policy    ‚îÇ    ‚îÇ ‚Ä¢ Generator ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Storage   ‚îÇ    ‚îÇ   Ratio     ‚îÇ    ‚îÇ ‚Ä¢ Reward    ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Batching  ‚îÇ    ‚îÇ ‚Ä¢ Losses    ‚îÇ    ‚îÇ   Computer  ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üîÑ PPO Training Flow

```mermaid
graph TD
    A[Input Prompts] --> B[Generate Sequences]
    B --> C[Extract SA Pairs]
    C --> D[Compute KL Divergence]
    D --> E[Get Rewards]
    E --> F[Adjust Rewards with KL]
    F --> G[Add Value Predictions]
    G --> H[Compute TD Errors]
    H --> I[Compute GAE Advantages]
    I --> J[Calculate Returns]
    J --> K[Train Value Model]
    K --> L[Train Policy Model]
    L --> M[Updated Models]
    
    style A fill:#e1f5fe
    style M fill:#c8e6c9
    style B fill:#fff3e0
    style L fill:#fff3e0
```

## üìÅ Project Structure

```
ppo_implementation/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ ppo_config.py          # PPO configuration and hyperparameters
‚îú‚îÄ‚îÄ base_setup/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base_setup.py          # Main setup orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer_setup.py     # Tokenizer initialization
‚îÇ   ‚îú‚îÄ‚îÄ lora_setup.py          # LoRA configuration
‚îÇ   ‚îî‚îÄ‚îÄ model_setup.py         # Base model setup
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base_model.py          # Base model class
‚îÇ   ‚îú‚îÄ‚îÄ sft_model.py           # Supervised Fine-Tuned model
‚îÇ   ‚îú‚îÄ‚îÄ policy_model.py        # Policy model (being trained)
‚îÇ   ‚îú‚îÄ‚îÄ reward_model.py        # Reward model
‚îÇ   ‚îî‚îÄ‚îÄ value_model.py         # Value function model
‚îú‚îÄ‚îÄ token_attribution/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ token_attribution.py   # Main token attribution
‚îÇ   ‚îú‚îÄ‚îÄ extractor.py           # Token extraction
‚îÇ   ‚îî‚îÄ‚îÄ sa_pair_builder.py     # State-action pair builder
‚îú‚îÄ‚îÄ reward_kl/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ kl_divergence.py       # KL divergence computation
‚îÇ   ‚îî‚îÄ‚îÄ reward_adjuster.py     # Reward adjustment with KL penalty
‚îú‚îÄ‚îÄ advantage/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ td_error.py            # Temporal difference errors
‚îÇ   ‚îú‚îÄ‚îÄ gae_advantage.py       # Generalized Advantage Estimation
‚îÇ   ‚îî‚îÄ‚îÄ return_calculator.py   # Return (reward-to-go) computation
‚îú‚îÄ‚îÄ experience_buffer/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ buffer.py              # Experience replay buffer
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ policy_ratio.py        # Policy ratio computation
‚îÇ   ‚îú‚îÄ‚îÄ policy_loss.py         # PPO-clip loss
‚îÇ   ‚îî‚îÄ‚îÄ value_loss.py          # Value function MSE loss
‚îú‚îÄ‚îÄ ppo_trainer/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ ppo_trainer.py         # Main PPO trainer
‚îÇ   ‚îú‚îÄ‚îÄ generator.py           # Sequence generation
‚îÇ   ‚îú‚îÄ‚îÄ reward_computer.py     # Reward computation
‚îÇ   ‚îú‚îÄ‚îÄ value_adder.py         # Value prediction addition
‚îÇ   ‚îî‚îÄ‚îÄ model_trainer.py       # Model training logic
‚îú‚îÄ‚îÄ main.py                    # Example usage
‚îî‚îÄ‚îÄ requirements.txt           # Dependencies
```

## üß† Component Details

### 1. Models Architecture

```mermaid
graph LR
    subgraph "Base Model"
        A[Qwen3-0.6B] --> B[LoRA Adapters]
    end
    
    subgraph "Model Variants"
        B --> C[SFT Model]
        B --> D[Policy Model]
        B --> E[Reward Model]
        B --> F[Value Model]
    end
    
    subgraph "Special Heads"
        E --> G[Reward Head]
        F --> H[Value Head]
    end
    
    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e8
    style E fill:#fff8e1
    style F fill:#fce4ec
```

### 2. Token Attribution Process

```mermaid
graph TD
    A[Input Prompt] --> B[Generate Token]
    B --> C[Extract State]
    C --> D[Record Action]
    D --> E[Compute Logprobs]
    E --> F[Build SA Pair]
    F --> G[Next Token]
    G --> B
    
    style A fill:#e1f5fe
    style F fill:#c8e6c9
```

### 3. PPO Training Components

```mermaid
graph LR
    subgraph "Policy Update"
        A[Old Policy] --> B[Generate Actions]
        B --> C[Compute New Policy]
        C --> D[Calculate Ratio]
        D --> E[PPO-Clip Loss]
    end
    
    subgraph "Value Update"
        F[Value Predictions] --> G[Target Returns]
        G --> H[MSE Loss]
    end
    
    subgraph "Advantage Estimation"
        I[Rewards] --> J[TD Errors]
        J --> K[GAE Advantages]
    end
    
    style E fill:#ffcdd2
    style H fill:#c8e6c9
    style K fill:#fff9c4
```

## üöÄ Quick Start

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd MLX-Week5-PPO

# Install dependencies
pip install -r requirements.txt
```

### Basic Usage

```python
from config import PPOConfig
from ppo_trainer import PPOTrainer

# Initialize configuration
config = PPOConfig()

# Create trainer
trainer = PPOTrainer(config)

# Example prompts
prompts = [
    "SUBREDDIT: r/relationships\nTITLE: Should I admit to snooping?\nPOST: ...\nTL;DR:",
    "SUBREDDIT: r/advice\nTITLE: Need help with decision\nPOST: ...\nTL;DR:"
]

# Run PPO step
results = trainer.ppo_step(prompts)

print(f"Generated texts: {results['generated_texts']}")
print(f"Rewards: {results['rewards']}")
print(f"Value loss: {results['value_loss']:.4f}")
print(f"Policy loss: {results['policy_loss']:.4f}")
```

## ‚öôÔ∏è Configuration

The `PPOConfig` class contains all hyperparameters:

```python
@dataclass
class PPOConfig:
    # Model configs
    model_name: str = "Qwen/Qwen3-0.6B-Base"
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    
    # Training configs
    learning_rate: float = 1e-5
    value_learning_rate: float = 1e-4
    batch_size: int = 32
    max_new_tokens: int = 10
    gamma: float = 0.99
    lam: float = 0.95
    kl_coef: float = 0.1
    clip_epsilon: float = 0.2
    
    # Generation configs
    temperature: float = 1.0
    top_k: int = 50
    top_p: float = 0.95
```

## üîß Key Components Explained

### 1. **Token Attribution**
- Divides generated sequences into (state, action) pairs
- Each token becomes an action, with previous tokens as state
- Enables token-level policy optimization

### 2. **KL Divergence Control**
- Prevents policy from deviating too far from reference model
- Uses KL penalty: `adjusted_reward = reward - Œ≤ √ó KL`
- Maintains generation quality during training

### 3. **Generalized Advantage Estimation (GAE)**
- Computes advantages using TD errors and discounting
- Balances bias and variance in advantage estimation
- Uses parameters Œ≥ (discount) and Œª (GAE parameter)

### 4. **PPO-Clip Loss**
- Clips policy ratio to prevent large updates
- `L = min(ratio √ó advantage, clip(ratio, 1-Œµ, 1+Œµ) √ó advantage)`
- Ensures stable policy updates

## üìä Training Metrics

The implementation tracks several key metrics:

- **Value Loss**: MSE between predicted and target values
- **Policy Loss**: PPO-clip loss for policy updates
- **KL Divergence**: Distance from reference model
- **Advantages**: GAE-computed advantages for each token
- **Rewards**: Sequence-level rewards from reward model

## üéØ Use Cases

This PPO implementation is designed for:

1. **Text Generation Fine-tuning**: Improve generation quality
2. **Summarization**: Train models to generate better summaries
3. **Dialogue Systems**: Optimize conversational responses
4. **Code Generation**: Improve code generation capabilities
5. **Creative Writing**: Enhance creative text generation

## üî¨ Extending the Implementation

### Adding New Components

1. **New Models**: Extend `BaseModel` class
2. **Custom Rewards**: Modify `RewardComputer`
3. **Different Advantages**: Implement new advantage estimators
4. **Alternative Losses**: Add new loss functions in `training/`

### Example: Custom Reward Model

```python
class CustomRewardModel(RewardModel):
    def forward(self, input_ids, attention_mask):
        # Custom reward computation
        base_reward = super().forward(input_ids, attention_mask)
        custom_penalty = self.compute_custom_penalty(input_ids)
        return base_reward - custom_penalty
```

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## üôè Acknowledgments

- Based on the PPO algorithm from "Proximal Policy Optimization Algorithms"
- Uses Hugging Face Transformers and PEFT for efficient fine-tuning
- Inspired by modern RLHF implementations

---

**Note**: This implementation is for educational and research purposes. For production use, consider additional safety measures and evaluation protocols.
